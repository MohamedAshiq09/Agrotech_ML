{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgroGraphNet: Graph Construction\n",
    "\n",
    "This notebook constructs farm network graphs based on spatial proximity, environmental similarity, and other relationships.\n",
    "\n",
    "## Objectives:\n",
    "1. Create spatial adjacency matrices based on farm locations\n",
    "2. Calculate environmental similarity between farms\n",
    "3. Build NetworkX graphs with node and edge features\n",
    "4. Convert to PyTorch Geometric format\n",
    "5. Create temporal graph sequences\n",
    "6. Analyze graph properties and connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from data_utils import *\n",
    "from graph_utils import *\n",
    "from visualization import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Loading processed data from: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "Load the preprocessed data from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "print(\"Loading processed datasets...\")\n",
    "\n",
    "# Load feature matrix (should be created by notebook 02)\n",
    "features_file = PROCESSED_DATA_DIR / 'features_scaled.csv'\n",
    "if features_file.exists():\n",
    "    features_df = pd.read_csv(features_file)\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "    print(f\"✅ Loaded feature matrix: {features_df.shape}\")\n",
    "else:\n",
    "    # If not available, load from raw data and create basic features\n",
    "    print(\"⚠️ Processed features not found. Loading raw data...\")\n",
    "    \n",
    "    # Load raw datasets\n",
    "    farm_files = list(FARM_LOCATIONS_DIR.glob('*.csv'))\n",
    "    weather_files = list(WEATHER_DIR.glob('*.csv'))\n",
    "    disease_files = list(DISEASE_LABELS_DIR.glob('*.csv'))\n",
    "    \n",
    "    if not all([farm_files, weather_files, disease_files]):\n",
    "        raise FileNotFoundError(\"Raw data files not found. Please run notebook 01 first.\")\n",
    "    \n",
    "    farms_df = pd.read_csv(farm_files[0])\n",
    "    weather_df = load_weather_data(str(weather_files[0]))\n",
    "    disease_df = load_disease_labels(str(disease_files[0]))\n",
    "    \n",
    "    # Create basic feature matrix\n",
    "    features_df = create_basic_feature_matrix(farms_df, weather_df, disease_df)\n",
    "    print(f\"✅ Created basic feature matrix: {features_df.shape}\")\n",
    "\n",
    "# Load farm locations\n",
    "farm_files = list(FARM_LOCATIONS_DIR.glob('*.csv'))\n",
    "if farm_files:\n",
    "    farms_df = pd.read_csv(farm_files[0])\n",
    "    print(f\"✅ Loaded farm locations: {len(farms_df)} farms\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Farm locations not found.\")\n",
    "\n",
    "print(f\"\\nData overview:\")\n",
    "print(f\"- Features shape: {features_df.shape}\")\n",
    "print(f\"- Time points: {features_df['date'].nunique()}\")\n",
    "print(f\"- Unique farms: {features_df['farm_id'].nunique()}\")\n",
    "print(f\"- Disease classes: {features_df['disease_type'].nunique() if 'disease_type' in features_df.columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_feature_matrix(farms_df, weather_df, disease_df):\n",
    "    \"\"\"\n",
    "    Create a basic feature matrix if processed features are not available\n",
    "    \"\"\"\n",
    "    print(\"Creating basic feature matrix...\")\n",
    "    \n",
    "    # Get unique time points\n",
    "    time_points = sorted(disease_df['date'].unique())\n",
    "    \n",
    "    feature_data = []\n",
    "    \n",
    "    for time_point in time_points:\n",
    "        for _, farm in farms_df.iterrows():\n",
    "            farm_id = farm['farm_id']\n",
    "            \n",
    "            # Basic farm features\n",
    "            farm_features = {\n",
    "                'farm_id': farm_id,\n",
    "                'date': time_point,\n",
    "                'lat': farm['lat'],\n",
    "                'lon': farm['lon'],\n",
    "                'area_hectares': farm['area_hectares']\n",
    "            }\n",
    "            \n",
    "            # Add crop type (one-hot encoded)\n",
    "            for crop_type in farms_df['crop_type'].unique():\n",
    "                farm_features[f'crop_{crop_type}'] = int(farm['crop_type'] == crop_type)\n",
    "            \n",
    "            # Add basic weather features\n",
    "            farm_weather = weather_df[\n",
    "                (abs(weather_df['lat'] - farm['lat']) < 0.01) & \n",
    "                (abs(weather_df['lon'] - farm['lon']) < 0.01) &\n",
    "                (weather_df['date'] == time_point)\n",
    "            ]\n",
    "            \n",
    "            if len(farm_weather) > 0:\n",
    "                farm_features['temperature'] = farm_weather['temperature'].mean()\n",
    "                farm_features['humidity'] = farm_weather['humidity'].mean()\n",
    "                farm_features['precipitation'] = farm_weather['precipitation'].mean()\n",
    "                farm_features['wind_speed'] = farm_weather['wind_speed'].mean()\n",
    "            else:\n",
    "                # Use overall averages\n",
    "                time_weather = weather_df[weather_df['date'] == time_point]\n",
    "                farm_features['temperature'] = time_weather['temperature'].mean()\n",
    "                farm_features['humidity'] = time_weather['humidity'].mean()\n",
    "                farm_features['precipitation'] = time_weather['precipitation'].mean()\n",
    "                farm_features['wind_speed'] = time_weather['wind_speed'].mean()\n",
    "            \n",
    "            # Add disease labels\n",
    "            farm_disease = disease_df[\n",
    "                (disease_df['farm_id'] == farm_id) & \n",
    "                (disease_df['date'] == time_point)\n",
    "            ]\n",
    "            \n",
    "            if len(farm_disease) > 0:\n",
    "                farm_features['disease_type'] = farm_disease['disease_type'].iloc[0]\n",
    "                disease_mapping = {'Healthy': 0, 'Blight': 1, 'Rust': 2, 'Mosaic': 3, 'Bacterial': 4}\n",
    "                farm_features['disease_label'] = disease_mapping.get(farm_disease['disease_type'].iloc[0], 0)\n",
    "                farm_features['severity'] = farm_disease['severity'].iloc[0]\n",
    "            else:\n",
    "                farm_features['disease_type'] = 'Healthy'\n",
    "                farm_features['disease_label'] = 0\n",
    "                farm_features['severity'] = 0.0\n",
    "            \n",
    "            feature_data.append(farm_features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Distance Matrix and Spatial Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance matrix between farms\n",
    "print(\"Creating distance matrix...\")\n",
    "\n",
    "distance_matrix = create_distance_matrix(farms_df)\n",
    "print(f\"✅ Distance matrix created: {distance_matrix.shape}\")\n",
    "\n",
    "# Analyze distance distribution\n",
    "distances_flat = distance_matrix[np.triu_indices_from(distance_matrix, k=1)]\n",
    "print(f\"Distance statistics:\")\n",
    "print(f\"- Mean distance: {distances_flat.mean():.2f} km\")\n",
    "print(f\"- Median distance: {np.median(distances_flat):.2f} km\")\n",
    "print(f\"- Max distance: {distances_flat.max():.2f} km\")\n",
    "print(f\"- Min distance: {distances_flat.min():.2f} km\")\n",
    "\n",
    "# Visualize distance distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(distances_flat, bins=30, alpha=0.7, color='skyblue')\n",
    "plt.axvline(GRAPH_CONFIG['distance_threshold_km'], color='red', linestyle='--', \n",
    "           label=f'Threshold: {GRAPH_CONFIG[\"distance_threshold_km\"]} km')\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Inter-Farm Distances')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(farms_df['lon'], farms_df['lat'], alpha=0.6, s=50)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Farm Locations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '03_distance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Distance analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adjacency matrix\n",
    "print(\"Creating adjacency matrix...\")\n",
    "\n",
    "adjacency_matrix = create_adjacency_matrix(\n",
    "    distance_matrix,\n",
    "    threshold_km=GRAPH_CONFIG['distance_threshold_km'],\n",
    "    min_neighbors=GRAPH_CONFIG['min_neighbors'],\n",
    "    max_neighbors=GRAPH_CONFIG['max_neighbors']\n",
    ")\n",
    "\n",
    "print(f\"✅ Adjacency matrix created: {adjacency_matrix.shape}\")\n",
    "\n",
    "# Analyze graph connectivity\n",
    "num_edges = adjacency_matrix.sum() // 2  # Divide by 2 for undirected graph\n",
    "num_nodes = adjacency_matrix.shape[0]\n",
    "density = num_edges / (num_nodes * (num_nodes - 1) / 2)\n",
    "avg_degree = adjacency_matrix.sum(axis=1).mean()\n",
    "\n",
    "print(f\"\\nGraph statistics:\")\n",
    "print(f\"- Nodes: {num_nodes}\")\n",
    "print(f\"- Edges: {num_edges}\")\n",
    "print(f\"- Density: {density:.4f}\")\n",
    "print(f\"- Average degree: {avg_degree:.2f}\")\n",
    "\n",
    "# Visualize degree distribution\n",
    "degrees = adjacency_matrix.sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(degrees, bins=range(int(degrees.min()), int(degrees.max()) + 2), \n",
    "         alpha=0.7, color='lightgreen', align='left')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Degree Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(adjacency_matrix, cmap='Blues', interpolation='nearest')\n",
    "plt.title('Adjacency Matrix')\n",
    "plt.xlabel('Farm Index')\n",
    "plt.ylabel('Farm Index')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '03_adjacency_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Adjacency analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Environmental Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate environmental similarity for each time point\n",
    "print(\"Calculating environmental similarity...\")\n",
    "\n",
    "time_points = sorted(features_df['date'].unique())\n",
    "environmental_similarities = {}\n",
    "\n",
    "for i, time_point in enumerate(time_points):\n",
    "    print(f\"Processing time point {i+1}/{len(time_points)}: {time_point}\")\n",
    "    \n",
    "    # Get data for this time point\n",
    "    time_data = features_df[features_df['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Select environmental features\n",
    "    env_features = ['temperature', 'humidity', 'precipitation', 'wind_speed']\n",
    "    available_features = [f for f in env_features if f in time_data.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        # Calculate environmental similarity\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        env_data = time_data[available_features].fillna(0)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        env_data_scaled = scaler.fit_transform(env_data)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        env_similarity = cosine_similarity(env_data_scaled)\n",
    "        \n",
    "        # Ensure similarity is between 0 and 1\n",
    "        env_similarity = (env_similarity + 1) / 2\n",
    "        \n",
    "    else:\n",
    "        # If no environmental features available, use uniform similarity\n",
    "        env_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    environmental_similarities[time_point] = env_similarity\n",
    "\n",
    "print(f\"\\n✅ Environmental similarity calculated for {len(time_points)} time points\")\n",
    "\n",
    "# Visualize environmental similarity for first time point\n",
    "first_time_point = time_points[0]\n",
    "first_similarity = environmental_similarities[first_time_point]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(first_similarity, cmap='viridis', interpolation='nearest')\n",
    "plt.title(f'Environmental Similarity Matrix\\n({first_time_point})')\n",
    "plt.xlabel('Farm Index')\n",
    "plt.ylabel('Farm Index')\n",
    "plt.colorbar(label='Similarity')\n",
    "plt.savefig(RESULTS_DIR / '03_environmental_similarity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Environmental similarity visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create NetworkX Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graphs for each time point\n",
    "print(\"Creating NetworkX graphs...\")\n",
    "\n",
    "networkx_graphs = {}\n",
    "\n",
    "for i, time_point in enumerate(time_points):\n",
    "    print(f\"Creating graph {i+1}/{len(time_points)}: {time_point}\")\n",
    "    \n",
    "    # Get data for this time point\n",
    "    time_data = features_df[features_df['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Prepare node features (exclude metadata)\n",
    "    exclude_cols = ['farm_id', 'date', 'disease_type', 'disease_label', 'severity']\n",
    "    feature_cols = [col for col in time_data.columns if col not in exclude_cols]\n",
    "    \n",
    "    node_features = time_data[feature_cols].fillna(0).values\n",
    "    \n",
    "    # Prepare edge features\n",
    "    edge_features = {\n",
    "        'distance': distance_matrix,\n",
    "        'environmental_similarity': environmental_similarities[time_point]\n",
    "    }\n",
    "    \n",
    "    # Create NetworkX graph\n",
    "    G = create_networkx_graph(\n",
    "        adjacency_matrix,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        farms_df\n",
    "    )\n",
    "    \n",
    "    networkx_graphs[time_point] = G\n",
    "\n",
    "print(f\"\\n✅ Created {len(networkx_graphs)} NetworkX graphs\")\n",
    "\n",
    "# Analyze graph properties for first graph\n",
    "first_graph = networkx_graphs[time_points[0]]\n",
    "graph_properties = analyze_graph_properties(first_graph)\n",
    "\n",
    "print(f\"\\nGraph properties (first time point):\")\n",
    "for prop, value in graph_properties.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {prop}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {prop}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph structure\n",
    "print(\"Creating graph visualizations...\")\n",
    "\n",
    "# Get disease labels for coloring\n",
    "first_time_data = features_df[features_df['date'] == time_points[0]].copy()\n",
    "first_time_data = first_time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "\n",
    "if 'disease_label' in first_time_data.columns:\n",
    "    node_labels = first_time_data['disease_label'].values\n",
    "else:\n",
    "    node_labels = None\n",
    "\n",
    "# Create graph visualization\n",
    "fig = plot_graph_network(\n",
    "    first_graph,\n",
    "    farms_df,\n",
    "    node_labels=node_labels,\n",
    "    save_path=str(RESULTS_DIR / '03_graph_visualization.png')\n",
    ")\n",
    "\n",
    "print(\"✅ Graph visualization created\")\n",
    "\n",
    "# Create interactive map showing graph connections\n",
    "print(\"Creating interactive graph map...\")\n",
    "\n",
    "center_lat = farms_df['lat'].mean()\n",
    "center_lon = farms_df['lon'].mean()\n",
    "\n",
    "import folium\n",
    "\n",
    "# Create base map\n",
    "m = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=10,\n",
    "    tiles='OpenStreetMap'\n",
    ")\n",
    "\n",
    "# Add farm nodes\n",
    "for i, (_, farm) in enumerate(farms_df.iterrows()):\n",
    "    # Color by disease status if available\n",
    "    if node_labels is not None:\n",
    "        disease_colors = {0: 'green', 1: 'red', 2: 'orange', 3: 'purple', 4: 'darkred'}\n",
    "        color = disease_colors.get(node_labels[i], 'blue')\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[farm['lat'], farm['lon']],\n",
    "        radius=8,\n",
    "        popup=f\"Farm: {farm['farm_id']}<br>Degree: {first_graph.degree(i)}\",\n",
    "        color='black',\n",
    "        fillColor=color,\n",
    "        fillOpacity=0.7,\n",
    "        weight=2\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add edges (sample of edges to avoid clutter)\n",
    "edges = list(first_graph.edges())\n",
    "sample_edges = edges[::max(1, len(edges)//50)]  # Sample every nth edge\n",
    "\n",
    "for edge in sample_edges:\n",
    "    farm1 = farms_df.iloc[edge[0]]\n",
    "    farm2 = farms_df.iloc[edge[1]]\n",
    "    \n",
    "    folium.PolyLine(\n",
    "        locations=[[farm1['lat'], farm1['lon']], [farm2['lat'], farm2['lon']]],\n",
    "        color='gray',\n",
    "        weight=1,\n",
    "        opacity=0.5\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map\n",
    "m.save(str(RESULTS_DIR / '03_graph_network_map.html'))\n",
    "print(f\"✅ Interactive graph map saved to {RESULTS_DIR / '03_graph_network_map.html'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to PyTorch Geometric Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NetworkX graphs to PyTorch Geometric format\n",
    "print(\"Converting to PyTorch Geometric format...\")\n",
    "\n",
    "pytorch_graphs = []\n",
    "\n",
    "for i, time_point in enumerate(time_points):\n",
    "    print(f\"Converting graph {i+1}/{len(time_points)}: {time_point}\")\n",
    "    \n",
    "    # Get NetworkX graph\n",
    "    G = networkx_graphs[time_point]\n",
    "    \n",
    "    # Get labels for this time point\n",
    "    time_data = features_df[features_df['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    if 'disease_label' in time_data.columns:\n",
    "        labels = time_data['disease_label'].values\n",
    "    else:\n",
    "        labels = np.zeros(len(time_data))  # Default to healthy\n",
    "    \n",
    "    # Convert to PyTorch Geometric\n",
    "    data = networkx_to_pytorch_geometric(G, labels)\n",
    "    \n",
    "    # Add metadata\n",
    "    data.time_point = time_point\n",
    "    data.num_farms = len(farms_df)\n",
    "    \n",
    "    pytorch_graphs.append(data)\n",
    "\n",
    "print(f\"\\n✅ Converted {len(pytorch_graphs)} graphs to PyTorch Geometric format\")\n",
    "\n",
    "# Display information about the graphs\n",
    "sample_graph = pytorch_graphs[0]\n",
    "print(f\"\\nSample graph information:\")\n",
    "print(f\"- Number of nodes: {sample_graph.x.shape[0]}\")\n",
    "print(f\"- Node feature dimension: {sample_graph.x.shape[1]}\")\n",
    "print(f\"- Number of edges: {sample_graph.edge_index.shape[1]}\")\n",
    "print(f\"- Edge feature dimension: {sample_graph.edge_attr.shape[1] if sample_graph.edge_attr is not None else 0}\")\n",
    "print(f\"- Number of classes: {len(torch.unique(sample_graph.y))}\")\n",
    "print(f\"- Class distribution: {torch.bincount(sample_graph.y).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all graph data\n",
    "print(\"Saving graph data...\")\n",
    "\n",
    "# Save processed features if not already saved\n",
    "if not (PROCESSED_DATA_DIR / 'features_scaled.csv').exists():\n",
    "    features_df.to_csv(PROCESSED_DATA_DIR / 'features_scaled.csv', index=False)\n",
    "    print(f\"✅ Features saved to {PROCESSED_DATA_DIR / 'features_scaled.csv'}\")\n",
    "\n",
    "# Save graph structures\n",
    "graph_data = {\n",
    "    'pytorch_graphs': pytorch_graphs,\n",
    "    'networkx_graphs': networkx_graphs,\n",
    "    'adjacency_matrix': adjacency_matrix,\n",
    "    'distance_matrix': distance_matrix,\n",
    "    'environmental_similarities': environmental_similarities,\n",
    "    'time_points': time_points,\n",
    "    'graph_properties': graph_properties,\n",
    "    'farms_df': farms_df\n",
    "}\n",
    "\n",
    "with open(GRAPHS_DIR / 'farm_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(graph_data, f)\n",
    "\n",
    "print(f\"✅ Graph data saved to {GRAPHS_DIR / 'farm_graphs.pkl'}\")\n",
    "\n",
    "# Save graph statistics\n",
    "graph_stats = {\n",
    "    'num_graphs': len(pytorch_graphs),\n",
    "    'num_nodes': sample_graph.x.shape[0],\n",
    "    'node_feature_dim': sample_graph.x.shape[1],\n",
    "    'num_edges': sample_graph.edge_index.shape[1],\n",
    "    'edge_feature_dim': sample_graph.edge_attr.shape[1] if sample_graph.edge_attr is not None else 0,\n",
    "    'num_classes': len(torch.unique(sample_graph.y)),\n",
    "    'graph_density': float(density),\n",
    "    'avg_degree': float(avg_degree),\n",
    "    'distance_threshold_km': GRAPH_CONFIG['distance_threshold_km'],\n",
    "    'min_neighbors': GRAPH_CONFIG['min_neighbors'],\n",
    "    'max_neighbors': GRAPH_CONFIG['max_neighbors']\n",
    "}\n",
    "\n",
    "with open(GRAPHS_DIR / 'graph_statistics.json', 'w') as f:\n",
    "    json.dump(graph_stats, f, indent=2)\n",
    "\n",
    "print(f\"✅ Graph statistics saved to {GRAPHS_DIR / 'graph_statistics.json'}\")\n",
    "\n",
    "print(\"\\n🎉 Graph construction completed successfully!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"- Created {len(pytorch_graphs)} temporal graphs\")\n",
    "print(f\"- Graph density: {density:.4f}\")\n",
    "print(f\"- Average degree: {avg_degree:.2f}\")\n",
    "print(f\"- Node features: {sample_graph.x.shape[1]}\")\n",
    "print(f\"- Edge features: {sample_graph.edge_attr.shape[1] if sample_graph.edge_attr is not None else 0}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run notebook 04_feature_engineering.ipynb\")\n",
    "print(\"2. Engineer additional features for better performance\")\n",
    "print(\"3. Prepare data for model training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
