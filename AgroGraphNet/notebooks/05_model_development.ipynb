# AgroGraphNet: Model Development - Complete Working Implementation

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, SAGEConv, GATConv, global_mean_pool
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import pickle
import json
import warnings
import networkx as nx
from scipy.spatial.distance import pdist, squareform
import time
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# Configuration Constants
RANDOM_SEED = 42
DISEASE_CLASSES = {
    0: 'Healthy',
    1: 'Fungal_Disease',
    2: 'Bacterial_Disease', 
    3: 'Viral_Disease',
    4: 'Pest_Infestation'
}

MODEL_CONFIG = {
    'hidden_dim': 128,
    'num_layers': 3,
    'dropout': 0.3,
    'learning_rate': 0.001,
    'batch_size': 32,
    'num_epochs': 100,
    'early_stopping_patience': 10
}

GRAPH_CONFIG = {
    'distance_threshold_km': 5.0,
    'min_neighbors': 2,
    'max_neighbors': 10
}

# Create directories
def create_directories():
    """Create necessary directories if they don't exist"""
    dirs = ['data/processed', 'models', 'results']
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

create_directories()

PROCESSED_DATA_DIR = Path('data/processed')
MODELS_DIR = Path('models')
RESULTS_DIR = Path('results')

# Set random seed for reproducibility
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def create_sample_data():
    """Create sample agricultural data if not available"""
    print("Creating sample agricultural data...")
    
    # Generate sample farms
    n_farms = 100
    n_time_points = 20
    
    # Create farm locations (latitude, longitude)
    np.random.seed(RANDOM_SEED)
    farms_data = []
    for i in range(n_farms):
        farms_data.append({
            'farm_id': f'FARM_{i:03d}',
            'latitude': 40.0 + np.random.normal(0, 0.1),  # Around 40°N
            'longitude': -74.0 + np.random.normal(0, 0.1),  # Around 74°W
            'farm_size': np.random.uniform(10, 500),  # hectares
            'crop_type': np.random.choice(['corn', 'wheat', 'soybean', 'rice'])
        })
    
    farms_df = pd.DataFrame(farms_data)
    
    # Generate time points
    start_date = datetime(2023, 1, 1)
    time_points = [(start_date + timedelta(weeks=i*2)).strftime('%Y-%m-%d') 
                   for i in range(n_time_points)]
    
    # Generate node features and labels for each time point
    n_features = 15
    feature_names = [
        'temperature', 'humidity', 'rainfall', 'soil_ph', 'soil_moisture',
        'nitrogen', 'phosphorus', 'potassium', 'organic_matter', 'elevation',
        'slope', 'aspect', 'irrigation', 'fertilizer_applied', 'pesticide_applied'
    ]
    
    node_features_by_time = {}
    labels_by_time = {}
    edge_features_by_time = {}
    
    for time_point in time_points:
        # Generate node features
        features = np.random.randn(n_farms, n_features)
        # Add some structure to make it more realistic
        features[:, 0] += 20  # temperature around 20°C
        features[:, 1] = np.abs(features[:, 1]) * 50 + 30  # humidity 30-130%
        features[:, 2] = np.abs(features[:, 2]) * 20  # rainfall 0-20mm
        features[:, 3] = features[:, 3] * 0.5 + 6.5  # soil pH around 6.5
        
        node_features_by_time[time_point] = features
        
        # Generate labels with some correlation to features
        disease_prob = 1 / (1 + np.exp(-(features[:, 1] - 80) / 10))  # High humidity -> disease
        labels = np.random.choice(len(DISEASE_CLASSES), size=n_farms, 
                                 p=[0.4, 0.2, 0.2, 0.1, 0.1])
        labels_by_time[time_point] = labels
        
        # Generate edge features (simplified)
        edge_features_by_time[time_point] = np.random.randn(n_farms, n_farms, 3)
    
    # Calculate distance matrix
    coords = farms_df[['latitude', 'longitude']].values
    distance_matrix = squareform(pdist(coords, metric='euclidean')) * 111  # Convert to km
    
    # Create feature summary
    summary_stats = {
        'total_farms': n_farms,
        'total_time_points': n_time_points,
        'feature_dimension': n_features,
        'disease_classes': len(DISEASE_CLASSES)
    }
    
    # Save data
    feature_data = {
        'node_features_by_time': node_features_by_time,
        'labels_by_time': labels_by_time,
        'edge_features_by_time': edge_features_by_time,
        'selected_features': feature_names,
        'time_points': time_points,
        'farms_df': farms_df,
        'distance_matrix': distance_matrix
    }
    
    with open(PROCESSED_DATA_DIR / 'engineered_features.pkl', 'wb') as f:
        pickle.dump(feature_data, f)
    
    with open(PROCESSED_DATA_DIR / 'feature_summary.json', 'w') as f:
        json.dump(summary_stats, f, indent=2)
    
    print("✅ Sample data created successfully!")
    return feature_data, summary_stats

def create_adjacency_matrix(distance_matrix, threshold_km=5.0, min_neighbors=2, max_neighbors=10):
    """Create adjacency matrix based on distance threshold"""
    n_nodes = distance_matrix.shape[0]
    adjacency = np.zeros((n_nodes, n_nodes))
    
    for i in range(n_nodes):
        # Get distances to all other nodes
        distances = distance_matrix[i]
        
        # Find neighbors within threshold
        neighbor_indices = np.where((distances <= threshold_km) & (distances > 0))[0]
        
        # Ensure minimum neighbors
        if len(neighbor_indices) < min_neighbors:
            # Add closest neighbors
            sorted_indices = np.argsort(distances)
            neighbor_indices = sorted_indices[1:min_neighbors+1]  # Exclude self
        
        # Limit maximum neighbors
        if len(neighbor_indices) > max_neighbors:
            # Keep only closest neighbors
            distances_subset = distances[neighbor_indices]
            closest_indices = np.argsort(distances_subset)[:max_neighbors]
            neighbor_indices = neighbor_indices[closest_indices]
        
        # Set adjacency
        adjacency[i, neighbor_indices] = 1
    
    # Make symmetric
    adjacency = np.maximum(adjacency, adjacency.T)
    
    return adjacency

def create_networkx_graph(adjacency_matrix, node_features, edge_features, farms_df):
    """Create NetworkX graph from adjacency matrix and features"""
    G = nx.Graph()
    
    # Add nodes with features
    for i in range(len(farms_df)):
        G.add_node(i, features=node_features[i], **farms_df.iloc[i].to_dict())
    
    # Add edges
    rows, cols = np.where(adjacency_matrix > 0)
    for i, j in zip(rows, cols):
        if i < j:  # Avoid duplicate edges
            edge_attr = edge_features[i, j] if edge_features.ndim == 3 else [1.0]
            G.add_edge(i, j, weight=edge_attr[0] if len(edge_attr) > 0 else 1.0)
    
    return G

def networkx_to_pytorch_geometric(G, labels):
    """Convert NetworkX graph to PyTorch Geometric Data object"""
    # Get node features
    node_features = np.array([G.nodes[i]['features'] for i in G.nodes()])
    x = torch.FloatTensor(node_features)
    
    # Get edge indices
    edge_index = torch.LongTensor(list(G.edges())).t().contiguous()
    
    # Get edge attributes (weights)
    edge_attr = torch.FloatTensor([G[u][v]['weight'] for u, v in G.edges()])
    edge_attr = edge_attr.unsqueeze(1)  # Add feature dimension
    
    # Get labels
    y = torch.LongTensor(labels)
    
    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)

# =============================================================================
# MODEL DEFINITIONS
# =============================================================================

class GCNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.3):
        super(GCNModel, self).__init__()
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        self.dropout = dropout
        
        # First layer
        self.convs.append(GCNConv(input_dim, hidden_dim))
        self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Hidden layers
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
            self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Output layer
        self.convs.append(GCNConv(hidden_dim, output_dim))
        
    def forward(self, x, edge_index, batch=None):
        # Apply graph convolutions
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = self.bns[i](x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        
        # Final layer
        x = self.convs[-1](x, edge_index)
        
        # Global pooling for graph-level prediction
        if batch is not None:
            x = global_mean_pool(x, batch)
        
        return F.log_softmax(x, dim=1)

class GraphSAGEModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.3):
        super(GraphSAGEModel, self).__init__()
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        self.dropout = dropout
        
        # First layer
        self.convs.append(SAGEConv(input_dim, hidden_dim))
        self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Hidden layers
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_dim, hidden_dim))
            self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Output layer
        self.convs.append(SAGEConv(hidden_dim, output_dim))
        
    def forward(self, x, edge_index, batch=None):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = self.bns[i](x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        
        x = self.convs[-1](x, edge_index)
        
        if batch is not None:
            x = global_mean_pool(x, batch)
        
        return F.log_softmax(x, dim=1)

class GATModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.3, heads=4):
        super(GATModel, self).__init__()
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        self.dropout = dropout
        self.heads = heads
        
        # First layer
        self.convs.append(GATConv(input_dim, hidden_dim // heads, heads=heads, dropout=dropout))
        self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Hidden layers
        for _ in range(num_layers - 2):
            self.convs.append(GATConv(hidden_dim, hidden_dim // heads, heads=heads, dropout=dropout))
            self.bns.append(nn.BatchNorm1d(hidden_dim))
        
        # Output layer
        self.convs.append(GATConv(hidden_dim, output_dim, heads=1, dropout=dropout))
        
    def forward(self, x, edge_index, batch=None):
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = self.bns[i](x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        
        x = self.convs[-1](x, edge_index)
        
        if batch is not None:
            x = global_mean_pool(x, batch)
        
        return F.log_softmax(x, dim=1)

# =============================================================================
# TRAINING AND EVALUATION FUNCTIONS
# =============================================================================

def train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs=100, 
                      learning_rate=0.001, early_stopping_patience=10, device='cpu'):
    """Train and evaluate a GNN model"""
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    criterion = nn.NLLLoss()
    
    train_losses = []
    val_accuracies = []
    best_val_acc = 0
    patience_counter = 0
    best_model_state = None
    
    print(f"Training {model.__class__.__name__}...")
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            out = model(batch.x, batch.edge_index, batch.batch)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                out = model(batch.x, batch.edge_index, batch.batch)
                pred = out.argmax(dim=1)
                val_correct += (pred == batch.y).sum().item()
                val_total += batch.y.size(0)
        
        val_acc = val_correct / val_total
        val_accuracies.append(val_acc)
        
        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch:3d}: Loss = {avg_loss:.4f}, Val Acc = {val_acc:.4f}")
        
        if patience_counter >= early_stopping_patience:
            print(f"Early stopping at epoch {epoch}")
            break
    
    # Load best model
    model.load_state_dict(best_model_state)
    
    # Test evaluation
    model.eval()
    test_predictions = []
    test_labels = []
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            out = model(batch.x, batch.edge_index, batch.batch)
            pred = out.argmax(dim=1)
            test_predictions.extend(pred.cpu().numpy())
            test_labels.extend(batch.y.cpu().numpy())
            test_correct += (pred == batch.y).sum().item()
            test_total += batch.y.size(0)
    
    test_acc = test_correct / test_total
    
    # Classification report
    test_predictions = np.array(test_predictions)
    test_labels = np.array(test_labels)
    
    class_names = [DISEASE_CLASSES[i] for i in range(len(DISEASE_CLASSES))]
    report = classification_report(test_labels, test_predictions, 
                                 target_names=class_names, output_dict=True)
    
    print(f"Final Test Accuracy: {test_acc:.4f}")
    
    return {
        'model_state_dict': best_model_state,
        'train_losses': train_losses,
        'val_accuracies': val_accuracies,
        'test_accuracy': test_acc,
        'test_predictions': test_predictions,
        'test_labels': test_labels,
        'classification_report': report,
        'best_val_accuracy': best_val_acc
    }

def create_baseline_models(X_train, y_train, X_test, y_test):
    """Create and train baseline models"""
    print("Training baseline models...")
    
    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_SEED),
        'SVM': SVC(kernel='rbf', random_state=RANDOM_SEED),
        'Logistic Regression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"Training {name}...")
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        results[name] = {
            'model': model,
            'test_accuracy': accuracy,
            'predictions': y_pred
        }
        
        print(f"{name} Accuracy: {accuracy:.4f}")
    
    return results

# =============================================================================
# VISUALIZATION FUNCTIONS
# =============================================================================

def plot_model_comparison(baseline_results, gnn_results, save_path=None):
    """Plot comparison of model performances"""
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # Prepare data
    model_names = list(baseline_results.keys()) + ['Best GNN']
    accuracies = [results['test_accuracy'] for results in baseline_results.values()] + [gnn_results['test_accuracy']]
    
    # Create colors
    colors = ['skyblue'] * len(baseline_results) + ['orange']
    
    # Create bar plot
    bars = ax.bar(model_names, accuracies, color=colors, alpha=0.7, edgecolor='black')
    
    # Add value labels on bars
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
    
    ax.set_ylabel('Test Accuracy', fontsize=12)
    ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
    ax.set_ylim(0, max(accuracies) * 1.1)
    
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')
    
    # Add grid
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"✅ Model comparison plot saved to {save_path}")
    
    plt.show()
    return fig

def plot_training_history(train_losses, val_accuracies, save_path=None):
    """Plot training history"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Plot training loss
    ax1.plot(train_losses, 'b-', linewidth=2, label='Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss Over Time')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # Plot validation accuracy
    ax2.plot(val_accuracies, 'r-', linewidth=2, label='Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Validation Accuracy Over Time')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"✅ Training history plot saved to {save_path}")
    
    plt.show()
    return fig

def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    
    # Create heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names, ax=ax)
    
    ax.set_xlabel('Predicted Label', fontsize=12)
    ax.set_ylabel('True Label', fontsize=12)
    ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
    
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"✅ Confusion matrix plot saved to {save_path}")
    
    plt.show()
    return fig

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function"""
    print("="*60)
    print("AgroGraphNet: Model Development")
    print("="*60)
    
    # 1. Load or create data
    try:
        print("Loading engineered features...")
        feature_file = PROCESSED_DATA_DIR / 'engineered_features.pkl'
        
        if not feature_file.exists():
            print("Engineered features not found. Creating sample data...")
            feature_data, summary_stats = create_sample_data()
        else:
            with open(feature_file, 'rb') as f:
                feature_data = pickle.load(f)
            
            with open(PROCESSED_DATA_DIR / 'feature_summary.json', 'r') as f:
                summary_stats = json.load(f)
        
        # Extract data
        node_features_by_time = feature_data['node_features_by_time']
        labels_by_time = feature_data['labels_by_time']
        edge_features_by_time = feature_data['edge_features_by_time']
        selected_features = feature_data['selected_features']
        time_points = feature_data['time_points']
        farms_df = feature_data['farms_df']
        distance_matrix = feature_data['distance_matrix']
        
        print(f"✅ Loaded data with {len(time_points)} time points and {len(farms_df)} farms")
        
    except Exception as e:
        print(f"Error loading data: {e}")
        return
    
    # 2. Create graphs
    try:
        print("\nCreating PyTorch Geometric graphs...")
        
        # Create adjacency matrix
        adjacency_matrix = create_adjacency_matrix(
            distance_matrix,
            threshold_km=GRAPH_CONFIG['distance_threshold_km'],
            min_neighbors=GRAPH_CONFIG['min_neighbors'],
            max_neighbors=GRAPH_CONFIG['max_neighbors']
        )
        
        print(f"✅ Adjacency matrix created: {adjacency_matrix.shape}")
        
        # Create graphs for each time point
        graphs = []
        for i, time_point in enumerate(time_points):
            node_features = node_features_by_time[time_point]
            labels = labels_by_time[time_point]
            edge_features = edge_features_by_time[time_point]
            
            # Create NetworkX graph
            G = create_networkx_graph(adjacency_matrix, node_features, edge_features, farms_df)
            
            # Convert to PyTorch Geometric
            data = networkx_to_pytorch_geometric(G, labels)
            data.time_point = time_point
            
            graphs.append(data)
        
        print(f"✅ Created {len(graphs)} PyTorch Geometric graphs")
        
    except Exception as e:
        print(f"Error creating graphs: {e}")
        return
    
    # 3. Data splitting
    try:
        print("\nSplitting data...")
        
        # Temporal split
        n_time_points = len(graphs)
        train_end = int(0.6 * n_time_points)
        val_end = int(0.8 * n_time_points)
        
        train_graphs = graphs[:train_end]
        val_graphs = graphs[train_end:val_end]
        test_graphs = graphs[val_end:]
        
        # Create data loaders
        batch_size = MODEL_CONFIG['batch_size']
        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)
        
        print(f"✅ Data split: {len(train_graphs)} train, {len(val_graphs)} val, {len(test_graphs)} test")
        
    except Exception as e:
        print(f"Error splitting data: {e}")
        return
    
    # 4. Train baseline models
    try:
        print("\nTraining baseline models...")
        
        # Prepare data for baseline models
        X_train = torch.cat([data.x for data in train_graphs]).numpy()
        y_train = torch.cat([data.y for data in train_graphs]).numpy()
        X_test = torch.cat([data.x for data in test_graphs]).numpy()
        y_test = torch.cat([data.y for data in test_graphs]).numpy()
        
        baseline_results = create_baseline_models(X_train, y_train, X_test, y_test)
        
        print("✅ Baseline models trained")
        
    except Exception as e:
        print(f"Error training baseline models: {e}")
        return
    
    # 5. Train GNN models
    try:
        print("\nTraining GNN models...")
        
        # Model parameters
        input_dim = graphs[0].x.shape[1]
        hidden_dim = MODEL_CONFIG['hidden_dim']
        output_dim = len(DISEASE_CLASSES)
        num_layers = MODEL_CONFIG['num_layers']
        dropout = MODEL_CONFIG['dropout']
        learning_rate = MODEL_CONFIG['learning_rate']
        num_epochs = MODEL_CONFIG['num_epochs']
        early_stopping_patience = MODEL_CONFIG['early_stopping_patience']
        
        gnn_results = {}
        
        # Train GCN
        print("\nTraining GCN...")
        gcn_model = GCNModel(input_dim, hidden_dim, output_dim, num_layers, dropout)
        gcn_results = train_and_evaluate(
            gcn_model, train_loader, val_loader, test_loader,
            num_epochs, learning_rate, early_stopping_patience, device
        )
        gnn_results['GCN'] = gcn_results
        
        # Train GraphSAGE
        print("\nTraining GraphSAGE...")
        sage_model = GraphSAGEModel(input_dim, hidden_dim, output_dim, num_layers, dropout)
        sage_results = train_and_evaluate(
            sage_model, train_loader, val_loader, test_loader,
            num_epochs, learning_rate, early_stopping_patience, device
        )
        gnn_results['GraphSAGE'] = sage_results
        
        # Train GAT
        print("\nTraining GAT...")
        gat_model = GATModel(input_dim, hidden_dim, output_dim, num_layers, dropout)
        gat_results = train_and_evaluate(
            gat_model, train_loader, val_loader, test_loader,
            num_epochs, learning_rate, early_stopping_patience, device
        )
        gnn_results['GAT'] = gat_results
        
        print("✅ All GNN models trained successfully!")
        
    except Exception as e:
        print(f"Error training GNN models: {e}")
        return
    
    # 6. Model comparison and results
    try:
        print("\nComparing model results...")
        
        # Combine baseline and GNN results
        all_results = {}
        
        # Add baseline results
        for model_name, results in baseline_results.items():
            all_results[model_name] = {
                'test_accuracy': results['test_accuracy'],
                'model_type': 'Baseline'
            }
        
        # Add GNN results
        for model_name, results in gnn_results.items():
            all_results[model_name] = {
                'test_accuracy': results['test_accuracy'],
                'model_type': 'GNN'
            }
        
        # Create comparison DataFrame
        comparison_df = pd.DataFrame([
            {'Model': name, 'Test Accuracy': results['test_accuracy'], 'Type': results['model_type']}
            for name, results in all_results.items()
        ]).sort_values('Test Accuracy', ascending=False)
        
        print("\nModel Performance Comparison:")
        print("=" * 60)
        print(comparison_df.to_string(index=False))
        
        # Find best models
        best_model_name = comparison_df.iloc[0]['Model']
        best_accuracy = comparison_df.iloc[0]['Test Accuracy']
        
        best_gnn = max(gnn_results.items(), key=lambda x: x[1]['test_accuracy'])
        best_gnn_name, best_gnn_results = best_gnn
        
        print(f"\n🏆 Best Overall Model: {best_model_name} (Accuracy: {best_accuracy:.4f})")
        print(f"🥇 Best GNN Model: {best_gnn_name} (Accuracy: {best_gnn_results['test_accuracy']:.4f})")
        
    except Exception as e:
        print(f"Error in model comparison: {e}")
        return
    
    # 7. Create visualizations
    try:
        print("\nCreating visualizations...")
        
        # Model comparison plot
        fig1 = plot_model_comparison(
            baseline_results, 
            best_gnn_results,
            save_path=str(RESULTS_DIR / 'model_comparison.png')
        )
        
        # Training history for best GNN model
        if 'train_losses' in best_gnn_results and 'val_accuracies' in best_gnn_results:
            fig2 = plot_training_history(
                best_gnn_results['train_losses'],
                best_gnn_results['val_accuracies'],
                save_path=str(RESULTS_DIR / f'{best_gnn_name}_training_history.png')
            )
        
        # Confusion matrix for best GNN model
        if 'test_labels' in best_gnn_results and 'test_predictions' in best_gnn_results:
            class_names = list(DISEASE_CLASSES.values())
            fig3 = plot_confusion_matrix(
                best_gnn_results['test_labels'],
                best_gnn_results['test_predictions'],
                class_names=class_names,
                save_path=str(RESULTS_DIR / f'{best_gnn_name}_confusion_matrix.png')
            )
        
        print("✅ Visualizations created and saved")
        
    except Exception as e:
        print(f"Error creating visualizations: {e}")
    
    # 8. Detailed analysis of best model
    try:
        print(f"\nDetailed Analysis of {best_gnn_name} Model:")
        print("=" * 50)
        
        # Classification report
        if 'classification_report' in best_gnn_results:
            report = best_gnn_results['classification_report']
            
            print("\nClassification Report:")
            class_names = list(DISEASE_CLASSES.values())
            
            # Create detailed report DataFrame
            report_data = []
            for class_name in class_names:
                if class_name in report:
                    report_data.append({
                        'Class': class_name,
                        'Precision': report[class_name]['precision'],
                        'Recall': report[class_name]['recall'],
                        'F1-Score': report[class_name]['f1-score'],
                        'Support': report[class_name]['support']
                    })
            
            if report_data:
                report_df = pd.DataFrame(report_data)
                print(report_df.to_string(index=False, float_format='%.3f'))
                
                # Overall metrics
                if 'macro avg' in report:
                    print(f"\nMacro Average:")
                    print(f"- Precision: {report['macro avg']['precision']:.3f}")
                    print(f"- Recall: {report['macro avg']['recall']:.3f}")
                    print(f"- F1-Score: {report['macro avg']['f1-score']:.3f}")
                
                if 'weighted avg' in report:
                    print(f"\nWeighted Average:")
                    print(f"- Precision: {report['weighted avg']['precision']:.3f}")
                    print(f"- Recall: {report['weighted avg']['recall']:.3f}")
                    print(f"- F1-Score: {report['weighted avg']['f1-score']:.3f}")
        
        # Performance by disease class
        if 'test_labels' in best_gnn_results and 'test_predictions' in best_gnn_results:
            test_labels = best_gnn_results['test_labels']
            test_predictions = best_gnn_results['test_predictions']
            
            print(f"\nPer-Class Accuracy:")
            for class_idx, class_name in DISEASE_CLASSES.items():
                class_mask = test_labels == class_idx
                if class_mask.sum() > 0:
                    class_accuracy = (test_predictions[class_mask] == test_labels[class_mask]).mean()
                    print(f"- {class_name}: {class_accuracy:.3f} ({class_mask.sum()} samples)")
        
        print("\n✅ Detailed analysis completed")
        
    except Exception as e:
        print(f"Error in detailed analysis: {e}")
    
    # 9. Save models and results
    try:
        print("\nSaving models and results...")
        
        # Save best GNN model
        best_model_path = MODELS_DIR / f'best_{best_gnn_name.lower()}_model.pth'
        torch.save(best_gnn_results['model_state_dict'], best_model_path)
        print(f"✅ Best model saved to {best_model_path}")
        
        # Save all results
        results_data = {
            'baseline_results': baseline_results,
            'gnn_results': gnn_results,
            'best_model_name': best_gnn_name,
            'best_model_accuracy': best_gnn_results['test_accuracy'],
            'model_comparison': comparison_df.to_dict('records'),
            'model_config': MODEL_CONFIG,
            'graph_config': GRAPH_CONFIG
        }
        
        with open(RESULTS_DIR / 'model_results.pkl', 'wb') as f:
            pickle.dump(results_data, f)
        
        print(f"✅ Results saved to {RESULTS_DIR / 'model_results.pkl'}")
        
        # Save model summary as JSON
        summary_data = {
            'best_model': best_gnn_name,
            'best_accuracy': float(best_gnn_results['test_accuracy']),
            'model_comparison': {
                name: float(results['test_accuracy']) 
                for name, results in all_results.items()
            },
            'training_config': MODEL_CONFIG,
            'dataset_info': summary_stats
        }
        
        with open(RESULTS_DIR / 'model_summary.json', 'w') as f:
            json.dump(summary_data, f, indent=2)
        
        print(f"✅ Model summary saved to {RESULTS_DIR / 'model_summary.json'}")
        
        # Save graphs for future use
        graph_data = {
            'graphs': graphs,
            'train_indices': list(range(len(train_graphs))),
            'val_indices': list(range(len(train_graphs), len(train_graphs) + len(val_graphs))),
            'test_indices': list(range(len(train_graphs) + len(val_graphs), len(graphs))),
            'adjacency_matrix': adjacency_matrix,
            'time_points': time_points
        }
        
        with open(PROCESSED_DATA_DIR / 'pytorch_graphs.pkl', 'wb') as f:
            pickle.dump(graph_data, f)
        
        print(f"✅ Graph data saved to {PROCESSED_DATA_DIR / 'pytorch_graphs.pkl'}")
        
    except Exception as e:
        print(f"Error saving results: {e}")
    
    # 10. Final summary
    print("\n" + "="*60)
    print("🎉 Model development completed successfully!")
    print("="*60)
    print("\nFinal Results Summary:")
    print(f"- Best Overall Model: {best_model_name} (Accuracy: {best_accuracy:.4f})")
    print(f"- Best GNN Model: {best_gnn_name} (Accuracy: {best_gnn_results['test_accuracy']:.4f})")
    
    best_baseline_acc = max(baseline_results.values(), key=lambda x: x['test_accuracy'])['test_accuracy']
    improvement = best_gnn_results['test_accuracy'] - best_baseline_acc
    print(f"- GNN improvement over best baseline: {improvement:.4f}")
    
    print(f"\nModel Comparison Summary:")
    for _, row in comparison_df.iterrows():
        print(f"- {row['Model']}: {row['Test Accuracy']:.4f} ({row['Type']})")
    
    print("\nFiles Created:")
    print(f"- Best model: {best_model_path}")
    print(f"- Results: {RESULTS_DIR / 'model_results.pkl'}")
    print(f"- Summary: {RESULTS_DIR / 'model_summary.json'}")
    print(f"- Graphs: {PROCESSED_DATA_DIR / 'pytorch_graphs.pkl'}")
    print(f"- Visualizations: {RESULTS_DIR}/")
    
    print("\nNext steps:")
    print("1. Analyze the generated visualizations")
    print("2. Fine-tune hyperparameters if needed")
    print("3. Create prediction maps and detailed reports")
    print("4. Deploy the best model for inference")

# Additional utility functions for enhanced functionality

def load_saved_model(model_class, model_path, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.3):
    """Load a saved model"""
    model = model_class(input_dim, hidden_dim, output_dim, num_layers, dropout)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model

def predict_with_model(model, data_loader, device='cpu'):
    """Make predictions with a trained model"""
    model.eval()
    predictions = []
    probabilities = []
    
    with torch.no_grad():
        for batch in data_loader:
            batch = batch.to(device)
            out = model(batch.x, batch.edge_index, batch.batch)
            pred = out.argmax(dim=1)
            prob = torch.exp(out)  # Convert from log probabilities
            
            predictions.extend(pred.cpu().numpy())
            probabilities.extend(prob.cpu().numpy())
    
    return np.array(predictions), np.array(probabilities)

def analyze_model_performance(y_true, y_pred, class_names):
    """Comprehensive model performance analysis"""
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
    
    # Basic metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)
    
    # Per-class analysis
    results = {
        'overall_accuracy': accuracy,
        'per_class_metrics': {}
    }
    
    for i, class_name in enumerate(class_names):
        results['per_class_metrics'][class_name] = {
            'precision': precision[i],
            'recall': recall[i],
            'f1_score': f1[i],
            'support': support[i]
        }
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    results['confusion_matrix'] = cm
    
    return results

def create_prediction_summary(predictions, probabilities, time_points, farms_df, disease_classes):
    """Create a summary of predictions"""
    summary_data = []
    
    for i, (pred, prob, time_point) in enumerate(zip(predictions, probabilities, time_points)):
        farm_info = farms_df.iloc[i % len(farms_df)]
        
        summary_data.append({
            'time_point': time_point,
            'farm_id': farm_info['farm_id'],
            'latitude': farm_info['latitude'],
            'longitude': farm_info['longitude'],
            'predicted_disease': disease_classes[pred],
            'confidence': prob[pred],
            'probabilities': {disease_classes[j]: prob[j] for j in range(len(disease_classes))}
        })
    
    return pd.DataFrame(summary_data)

# Run the main function if script is executed directly
if __name__ == "__main__":
    main()