{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgroGraphNet: Feature Engineering\n",
    "\n",
    "This notebook creates comprehensive features for machine learning including node features, edge features, and temporal patterns.\n",
    "\n",
    "## Objectives:\n",
    "1. Create node features from satellite, weather, and farm data\n",
    "2. Engineer edge features based on spatial and environmental relationships\n",
    "3. Create temporal features and sequences\n",
    "4. Prepare feature matrices for GNN training\n",
    "5. Feature selection and importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Loading processed data from: C:\\Users\\Ash09\\agro_tech\\AgroGraphNet\\notebooks\\..\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from data_utils import *\n",
    "from graph_utils import *\n",
    "from visualization import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Loading processed data from: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed datasets...\n",
      "âœ… Loaded feature matrix: (1200, 37)\n",
      "âœ… Loaded farm locations: 100 farms\n",
      "âš ï¸ Graph data not found. Will create graphs in this notebook.\n",
      "\n",
      "Data overview:\n",
      "- Features shape: (1200, 37)\n",
      "- Time points: 12\n",
      "- Unique farms: 100\n",
      "- Disease classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Load processed data from previous notebooks\n",
    "print(\"Loading processed datasets...\")\n",
    "\n",
    "# Load feature matrix\n",
    "features_file = PROCESSED_DATA_DIR / 'features_scaled.csv'\n",
    "if features_file.exists():\n",
    "    features_df = pd.read_csv(features_file)\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "    print(f\"âœ… Loaded feature matrix: {features_df.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Processed features not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "# Load farm locations\n",
    "farm_files = list(FARM_LOCATIONS_DIR.glob('*.csv'))\n",
    "if farm_files:\n",
    "    farms_df = pd.read_csv(farm_files[0])\n",
    "    print(f\"âœ… Loaded farm locations: {len(farms_df)} farms\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Farm locations not found.\")\n",
    "\n",
    "# Load graph data if available\n",
    "graph_file = GRAPHS_DIR / 'farm_graphs.pkl'\n",
    "if graph_file.exists():\n",
    "    import pickle\n",
    "    with open(graph_file, 'rb') as f:\n",
    "        graph_data = pickle.load(f)\n",
    "    print(f\"âœ… Loaded graph data: {len(graph_data)} time steps\")\n",
    "else:\n",
    "    print(\"âš ï¸ Graph data not found. Will create graphs in this notebook.\")\n",
    "    graph_data = None\n",
    "\n",
    "print(f\"\\nData overview:\")\n",
    "print(f\"- Features shape: {features_df.shape}\")\n",
    "print(f\"- Time points: {features_df['date'].nunique()}\")\n",
    "print(f\"- Unique farms: {features_df['farm_id'].nunique()}\")\n",
    "print(f\"- Disease classes: {features_df['disease_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Node Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering node features...\n",
      "Base feature columns: 31\n",
      "âœ… Created vegetation health score\n",
      "âœ… Created weather stress indicators\n",
      "âœ… Created cyclical month features\n",
      "âœ… Created farm size categories\n",
      "Creating historical disease features...\n",
      "âœ… Created historical disease features\n",
      "\n",
      "Total engineered features: 46\n",
      "Added 15 new features\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive node features\n",
    "print(\"Engineering node features...\")\n",
    "\n",
    "# Identify feature columns\n",
    "exclude_cols = ['farm_id', 'date', 'disease_type', 'disease_label', 'severity', 'is_diseased']\n",
    "feature_columns = [col for col in features_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Base feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Create additional engineered features\n",
    "features_engineered = features_df.copy()\n",
    "\n",
    "# 1. Vegetation health indicators\n",
    "if 'vegetation_NDVI' in features_df.columns and 'vegetation_EVI' in features_df.columns:\n",
    "    features_engineered['vegetation_health_score'] = (\n",
    "        features_df['vegetation_NDVI'] + features_df['vegetation_EVI']\n",
    "    ) / 2\n",
    "    print(\"âœ… Created vegetation health score\")\n",
    "\n",
    "# 2. Weather stress indicators\n",
    "weather_cols = [col for col in features_df.columns if col.startswith('weather_')]\n",
    "if len(weather_cols) > 0:\n",
    "    # Temperature stress (deviation from optimal)\n",
    "    if 'weather_temperature' in features_df.columns:\n",
    "        optimal_temp = 20  # Celsius\n",
    "        features_engineered['temperature_stress'] = np.abs(\n",
    "            features_df['weather_temperature'] - optimal_temp\n",
    "        )\n",
    "    \n",
    "    # Humidity stress\n",
    "    if 'weather_humidity' in features_df.columns:\n",
    "        optimal_humidity = 60  # Percentage\n",
    "        features_engineered['humidity_stress'] = np.abs(\n",
    "            features_df['weather_humidity'] - optimal_humidity\n",
    "        )\n",
    "    \n",
    "    # Weather variability (if rolling features exist)\n",
    "    rolling_std_cols = [col for col in features_df.columns if 'rolling_std' in col]\n",
    "    if rolling_std_cols:\n",
    "        features_engineered['weather_variability'] = features_df[rolling_std_cols].mean(axis=1)\n",
    "    \n",
    "    print(\"âœ… Created weather stress indicators\")\n",
    "\n",
    "# 3. Seasonal features\n",
    "if 'month' in features_df.columns:\n",
    "    # Cyclical encoding for month\n",
    "    features_engineered['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_engineered['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    print(\"âœ… Created cyclical month features\")\n",
    "\n",
    "# 4. Farm size categories\n",
    "if 'area_hectares' in features_df.columns:\n",
    "    area_quartiles = features_df['area_hectares'].quantile([0.25, 0.5, 0.75])\n",
    "    features_engineered['farm_size_category'] = pd.cut(\n",
    "        features_df['area_hectares'],\n",
    "        bins=[-np.inf, area_quartiles[0.25], area_quartiles[0.5], area_quartiles[0.75], np.inf],\n",
    "        labels=['Small', 'Medium', 'Large', 'Very Large']\n",
    "    )\n",
    "    \n",
    "    # One-hot encode farm size categories\n",
    "    size_dummies = pd.get_dummies(features_engineered['farm_size_category'], prefix='size')\n",
    "    features_engineered = pd.concat([features_engineered, size_dummies], axis=1)\n",
    "    features_engineered.drop('farm_size_category', axis=1, inplace=True)\n",
    "    print(\"âœ… Created farm size categories\")\n",
    "\n",
    "# 5. Historical disease features (lag features)\n",
    "print(\"Creating historical disease features...\")\n",
    "features_engineered = features_engineered.sort_values(['farm_id', 'date'])\n",
    "\n",
    "# Previous disease status (1 time step lag)\n",
    "features_engineered['prev_disease_label'] = features_engineered.groupby('farm_id')['disease_label'].shift(1)\n",
    "features_engineered['prev_severity'] = features_engineered.groupby('farm_id')['severity'].shift(1)\n",
    "features_engineered['prev_is_diseased'] = features_engineered.groupby('farm_id')['is_diseased'].shift(1)\n",
    "\n",
    "# Fill NaN values for first time step\n",
    "features_engineered['prev_disease_label'].fillna(0, inplace=True)  # Assume healthy initially\n",
    "features_engineered['prev_severity'].fillna(0, inplace=True)\n",
    "features_engineered['prev_is_diseased'].fillna(0, inplace=True)\n",
    "\n",
    "# Disease history (cumulative)\n",
    "features_engineered['disease_history_count'] = features_engineered.groupby('farm_id')['is_diseased'].cumsum()\n",
    "features_engineered['avg_historical_severity'] = features_engineered.groupby('farm_id')['severity'].expanding().mean().reset_index(0, drop=True)\n",
    "\n",
    "print(\"âœ… Created historical disease features\")\n",
    "\n",
    "# Update feature columns list\n",
    "new_feature_columns = [col for col in features_engineered.columns if col not in exclude_cols]\n",
    "print(f\"\\nTotal engineered features: {len(new_feature_columns)}\")\n",
    "print(f\"Added {len(new_feature_columns) - len(feature_columns)} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edge Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering edge features...\n",
      "âœ… Distance matrix created: (100, 100)\n",
      "Processing time point: 2023-01-31 00:00:00\n",
      "Processing time point: 2023-02-28 00:00:00\n",
      "Processing time point: 2023-03-31 00:00:00\n",
      "Processing time point: 2023-04-30 00:00:00\n",
      "Processing time point: 2023-05-31 00:00:00\n",
      "Processing time point: 2023-06-30 00:00:00\n",
      "Processing time point: 2023-07-31 00:00:00\n",
      "Processing time point: 2023-08-31 00:00:00\n",
      "Processing time point: 2023-09-30 00:00:00\n",
      "Processing time point: 2023-10-31 00:00:00\n",
      "Processing time point: 2023-11-30 00:00:00\n",
      "Processing time point: 2023-12-31 00:00:00\n",
      "âœ… Edge features created for 12 time points\n",
      "Edge feature types: ['distance', 'environmental_similarity', 'vegetation_similarity', 'crop_similarity', 'combined_similarity']\n"
     ]
    }
   ],
   "source": [
    "# Create edge features for graph construction\n",
    "print(\"Engineering edge features...\")\n",
    "\n",
    "# Calculate distance matrix\n",
    "distance_matrix = create_distance_matrix(farms_df)\n",
    "print(f\"âœ… Distance matrix created: {distance_matrix.shape}\")\n",
    "\n",
    "# Calculate environmental similarity for each time point\n",
    "time_points = sorted(features_engineered['date'].unique())\n",
    "edge_features_by_time = {}\n",
    "\n",
    "for time_point in time_points:\n",
    "    print(f\"Processing time point: {time_point}\")\n",
    "    \n",
    "    # Get data for this time point\n",
    "    time_data = features_engineered[features_engineered['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Environmental similarity based on weather features\n",
    "    weather_features = [col for col in time_data.columns if col.startswith('weather_')]\n",
    "    if weather_features:\n",
    "        weather_data = time_data[weather_features].values\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        env_similarity = cosine_similarity(weather_data)\n",
    "    else:\n",
    "        env_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Vegetation similarity\n",
    "    vegetation_features = [col for col in time_data.columns if col.startswith('vegetation_')]\n",
    "    if vegetation_features:\n",
    "        vegetation_data = time_data[vegetation_features].values\n",
    "        vegetation_similarity = cosine_similarity(vegetation_data)\n",
    "    else:\n",
    "        vegetation_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Crop type similarity\n",
    "    crop_features = [col for col in time_data.columns if col.startswith('crop_')]\n",
    "    if crop_features:\n",
    "        crop_data = time_data[crop_features].values\n",
    "        crop_similarity = cosine_similarity(crop_data)\n",
    "    else:\n",
    "        crop_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Combine edge features\n",
    "    edge_features = {\n",
    "        'distance': distance_matrix,\n",
    "        'environmental_similarity': env_similarity,\n",
    "        'vegetation_similarity': vegetation_similarity,\n",
    "        'crop_similarity': crop_similarity,\n",
    "        'combined_similarity': (env_similarity + vegetation_similarity + crop_similarity) / 3\n",
    "    }\n",
    "    \n",
    "    edge_features_by_time[time_point] = edge_features\n",
    "\n",
    "print(f\"âœ… Edge features created for {len(time_points)} time points\")\n",
    "print(f\"Edge feature types: {list(edge_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing feature importance...\n",
      "Feature matrix shape: (1200, 46)\n",
      "Target distribution: {0: 867, 2: 113, 1: 99, 3: 75, 4: 46}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Winter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Feature selection using ANOVA F-test\u001b[39;00m\n\u001b[32m     23\u001b[39m selector = SelectKBest(score_func=f_classif, k=\u001b[38;5;28mmin\u001b[39m(\u001b[32m50\u001b[39m, X.shape[\u001b[32m1\u001b[39m]))  \u001b[38;5;66;03m# Select top 50 features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X_selected = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m selected_features = X.columns[selector.get_support()].tolist()\n\u001b[32m     26\u001b[39m feature_scores = selector.scores_[selector.get_support()]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:897\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:563\u001b[39m, in \u001b[36m_BaseFilter.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    561\u001b[39m     X = validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse=[\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params(X, y)\n\u001b[32m    568\u001b[39m score_func_ret = \u001b[38;5;28mself\u001b[39m.score_func(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:971\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    970\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    973\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6662\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6656\u001b[39m     results = [\n\u001b[32m   6657\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6658\u001b[39m     ]\n\u001b[32m   6660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6661\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6663\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Winter'"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance for disease prediction\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Prepare data for feature selection\n",
    "feature_cols = [col for col in features_engineered.columns if col not in exclude_cols]\n",
    "X = features_engineered[feature_cols].fillna(0)\n",
    "y = features_engineered['disease_label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Remove constant features\n",
    "constant_features = []\n",
    "for col in X.columns:\n",
    "    if X[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "if constant_features:\n",
    "    print(f\"Removing {len(constant_features)} constant features\")\n",
    "    X = X.drop(columns=constant_features)\n",
    "\n",
    "# Feature selection using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=min(50, X.shape[1]))  # Select top 50 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"\\nâœ… Selected {len(selected_features)} most important features\")\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance_score': feature_scores\n",
    "}).sort_values('importance_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "display(feature_importance.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance_score'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score (F-statistic)')\n",
    "plt.title('Top 20 Feature Importance for Disease Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '04_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(PROCESSED_DATA_DIR / 'feature_importance.csv', index=False)\n",
    "print(f\"âœ… Feature importance saved to {PROCESSED_DATA_DIR / 'feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Final Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final feature matrices for GNN training\n",
    "print(\"Creating final feature matrices...\")\n",
    "\n",
    "# Use selected features for final matrix\n",
    "final_features = features_engineered[['farm_id', 'date'] + selected_features + \n",
    "                                   ['disease_type', 'disease_label', 'severity', 'is_diseased']].copy()\n",
    "\n",
    "print(f\"Final feature matrix shape: {final_features.shape}\")\n",
    "\n",
    "# Scale the selected features\n",
    "scaler = StandardScaler()\n",
    "final_features[selected_features] = scaler.fit_transform(final_features[selected_features])\n",
    "\n",
    "print(\"âœ… Features scaled using StandardScaler\")\n",
    "\n",
    "# Create node feature matrices for each time point\n",
    "node_features_by_time = {}\n",
    "labels_by_time = {}\n",
    "\n",
    "for time_point in time_points:\n",
    "    time_data = final_features[final_features['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Node features (exclude metadata and labels)\n",
    "    node_features = time_data[selected_features].values\n",
    "    labels = time_data['disease_label'].values\n",
    "    \n",
    "    node_features_by_time[time_point] = node_features\n",
    "    labels_by_time[time_point] = labels\n",
    "\n",
    "print(f\"âœ… Node features created for {len(time_points)} time points\")\n",
    "print(f\"Node feature dimension: {node_features.shape[1]}\")\n",
    "\n",
    "# Save processed features\n",
    "final_features.to_csv(PROCESSED_DATA_DIR / 'final_features.csv', index=False)\n",
    "print(f\"âœ… Final features saved to {PROCESSED_DATA_DIR / 'final_features.csv'}\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, PROCESSED_DATA_DIR / 'feature_scaler.pkl')\n",
    "print(f\"âœ… Feature scaler saved to {PROCESSED_DATA_DIR / 'feature_scaler.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns in features\n",
    "print(\"Analyzing temporal patterns...\")\n",
    "\n",
    "# Calculate feature stability over time\n",
    "feature_stability = {}\n",
    "\n",
    "for feature in selected_features[:10]:  # Analyze top 10 features\n",
    "    feature_values_by_time = []\n",
    "    \n",
    "    for time_point in time_points:\n",
    "        time_data = final_features[final_features['date'] == time_point]\n",
    "        feature_values_by_time.append(time_data[feature].mean())\n",
    "    \n",
    "    # Calculate coefficient of variation (stability measure)\n",
    "    cv = np.std(feature_values_by_time) / (np.mean(feature_values_by_time) + 1e-8)\n",
    "    feature_stability[feature] = cv\n",
    "\n",
    "# Visualize temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Disease progression over time\n",
    "disease_progression = final_features.groupby('date')['is_diseased'].mean()\n",
    "axes[0, 0].plot(disease_progression.index, disease_progression.values, 'o-', color='red')\n",
    "axes[0, 0].set_title('Disease Prevalence Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Proportion of Diseased Farms')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Feature stability\n",
    "stability_df = pd.DataFrame(list(feature_stability.items()), columns=['Feature', 'CV'])\n",
    "stability_df = stability_df.sort_values('CV')\n",
    "axes[0, 1].barh(range(len(stability_df)), stability_df['CV'])\n",
    "axes[0, 1].set_yticks(range(len(stability_df)))\n",
    "axes[0, 1].set_yticklabels(stability_df['Feature'])\n",
    "axes[0, 1].set_xlabel('Coefficient of Variation')\n",
    "axes[0, 1].set_title('Feature Temporal Stability')\n",
    "\n",
    "# 3. Vegetation health over time\n",
    "if 'vegetation_health_score' in final_features.columns:\n",
    "    veg_health = final_features.groupby('date')['vegetation_health_score'].mean()\n",
    "    axes[1, 0].plot(veg_health.index, veg_health.values, 'o-', color='green')\n",
    "    axes[1, 0].set_title('Average Vegetation Health Over Time')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Vegetation Health Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Weather stress over time\n",
    "if 'temperature_stress' in final_features.columns:\n",
    "    temp_stress = final_features.groupby('date')['temperature_stress'].mean()\n",
    "    axes[1, 1].plot(temp_stress.index, temp_stress.values, 'o-', color='orange')\n",
    "    axes[1, 1].set_title('Average Temperature Stress Over Time')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Temperature Stress')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '04_temporal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Temporal analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all engineered data for next notebooks\n",
    "print(\"Saving engineered features...\")\n",
    "\n",
    "# Save node features and labels by time\n",
    "import pickle\n",
    "\n",
    "feature_data = {\n",
    "    'node_features_by_time': node_features_by_time,\n",
    "    'labels_by_time': labels_by_time,\n",
    "    'edge_features_by_time': edge_features_by_time,\n",
    "    'selected_features': selected_features,\n",
    "    'feature_importance': feature_importance,\n",
    "    'time_points': time_points,\n",
    "    'farms_df': farms_df,\n",
    "    'distance_matrix': distance_matrix\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DATA_DIR / 'engineered_features.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_data, f)\n",
    "\n",
    "print(f\"âœ… Engineered features saved to {PROCESSED_DATA_DIR / 'engineered_features.pkl'}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_features': len(selected_features),\n",
    "    'total_farms': len(farms_df),\n",
    "    'total_time_points': len(time_points),\n",
    "    'disease_classes': len(final_features['disease_type'].unique()),\n",
    "    'feature_dimension': node_features.shape[1],\n",
    "    'total_samples': len(final_features)\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DATA_DIR / 'feature_summary.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Summary statistics saved to {PROCESSED_DATA_DIR / 'feature_summary.json'}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Feature engineering completed successfully!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"- Total engineered features: {len(selected_features)}\")\n",
    "print(f\"- Node feature dimension: {node_features.shape[1]}\")\n",
    "print(f\"- Time points: {len(time_points)}\")\n",
    "print(f\"- Total samples: {len(final_features)}\")\n",
    "print(f\"- Disease classes: {len(final_features['disease_type'].unique())}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run notebook 05_model_development.ipynb\")\n",
    "print(\"2. Train GNN models using engineered features\")\n",
    "print(\"3. Compare with baseline models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
