{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgroGraphNet: Feature Engineering\n",
    "\n",
    "This notebook creates comprehensive features for machine learning including node features, edge features, and temporal patterns.\n",
    "\n",
    "## Objectives:\n",
    "1. Create node features from satellite, weather, and farm data\n",
    "2. Engineer edge features based on spatial and environmental relationships\n",
    "3. Create temporal features and sequences\n",
    "4. Prepare feature matrices for GNN training\n",
    "5. Feature selection and importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from data_utils import *\n",
    "from graph_utils import *\n",
    "from visualization import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Loading processed data from: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from previous notebooks\n",
    "print(\"Loading processed datasets...\")\n",
    "\n",
    "# Load feature matrix\n",
    "features_file = PROCESSED_DATA_DIR / 'features_scaled.csv'\n",
    "if features_file.exists():\n",
    "    features_df = pd.read_csv(features_file)\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "    print(f\"âœ… Loaded feature matrix: {features_df.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Processed features not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "# Load farm locations\n",
    "farm_files = list(FARM_LOCATIONS_DIR.glob('*.csv'))\n",
    "if farm_files:\n",
    "    farms_df = pd.read_csv(farm_files[0])\n",
    "    print(f\"âœ… Loaded farm locations: {len(farms_df)} farms\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Farm locations not found.\")\n",
    "\n",
    "# Load graph data if available\n",
    "graph_file = GRAPHS_DIR / 'farm_graphs.pkl'\n",
    "if graph_file.exists():\n",
    "    import pickle\n",
    "    with open(graph_file, 'rb') as f:\n",
    "        graph_data = pickle.load(f)\n",
    "    print(f\"âœ… Loaded graph data: {len(graph_data)} time steps\")\n",
    "else:\n",
    "    print(\"âš ï¸ Graph data not found. Will create graphs in this notebook.\")\n",
    "    graph_data = None\n",
    "\n",
    "print(f\"\\nData overview:\")\n",
    "print(f\"- Features shape: {features_df.shape}\")\n",
    "print(f\"- Time points: {features_df['date'].nunique()}\")\n",
    "print(f\"- Unique farms: {features_df['farm_id'].nunique()}\")\n",
    "print(f\"- Disease classes: {features_df['disease_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Node Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive node features\n",
    "print(\"Engineering node features...\")\n",
    "\n",
    "# Identify feature columns\n",
    "exclude_cols = ['farm_id', 'date', 'disease_type', 'disease_label', 'severity', 'is_diseased']\n",
    "feature_columns = [col for col in features_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Base feature columns: {len(feature_columns)}\")\n",
    "\n",
    "# Create additional engineered features\n",
    "features_engineered = features_df.copy()\n",
    "\n",
    "# 1. Vegetation health indicators\n",
    "if 'vegetation_NDVI' in features_df.columns and 'vegetation_EVI' in features_df.columns:\n",
    "    features_engineered['vegetation_health_score'] = (\n",
    "        features_df['vegetation_NDVI'] + features_df['vegetation_EVI']\n",
    "    ) / 2\n",
    "    print(\"âœ… Created vegetation health score\")\n",
    "\n",
    "# 2. Weather stress indicators\n",
    "weather_cols = [col for col in features_df.columns if col.startswith('weather_')]\n",
    "if len(weather_cols) > 0:\n",
    "    # Temperature stress (deviation from optimal)\n",
    "    if 'weather_temperature' in features_df.columns:\n",
    "        optimal_temp = 20  # Celsius\n",
    "        features_engineered['temperature_stress'] = np.abs(\n",
    "            features_df['weather_temperature'] - optimal_temp\n",
    "        )\n",
    "    \n",
    "    # Humidity stress\n",
    "    if 'weather_humidity' in features_df.columns:\n",
    "        optimal_humidity = 60  # Percentage\n",
    "        features_engineered['humidity_stress'] = np.abs(\n",
    "            features_df['weather_humidity'] - optimal_humidity\n",
    "        )\n",
    "    \n",
    "    # Weather variability (if rolling features exist)\n",
    "    rolling_std_cols = [col for col in features_df.columns if 'rolling_std' in col]\n",
    "    if rolling_std_cols:\n",
    "        features_engineered['weather_variability'] = features_df[rolling_std_cols].mean(axis=1)\n",
    "    \n",
    "    print(\"âœ… Created weather stress indicators\")\n",
    "\n",
    "# 3. Seasonal features\n",
    "if 'month' in features_df.columns:\n",
    "    # Cyclical encoding for month\n",
    "    features_engineered['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_engineered['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    print(\"âœ… Created cyclical month features\")\n",
    "\n",
    "# 4. Farm size categories\n",
    "if 'area_hectares' in features_df.columns:\n",
    "    area_quartiles = features_df['area_hectares'].quantile([0.25, 0.5, 0.75])\n",
    "    features_engineered['farm_size_category'] = pd.cut(\n",
    "        features_df['area_hectares'],\n",
    "        bins=[-np.inf, area_quartiles[0.25], area_quartiles[0.5], area_quartiles[0.75], np.inf],\n",
    "        labels=['Small', 'Medium', 'Large', 'Very Large']\n",
    "    )\n",
    "    \n",
    "    # One-hot encode farm size categories\n",
    "    size_dummies = pd.get_dummies(features_engineered['farm_size_category'], prefix='size')\n",
    "    features_engineered = pd.concat([features_engineered, size_dummies], axis=1)\n",
    "    features_engineered.drop('farm_size_category', axis=1, inplace=True)\n",
    "    print(\"âœ… Created farm size categories\")\n",
    "\n",
    "# 5. Historical disease features (lag features)\n",
    "print(\"Creating historical disease features...\")\n",
    "features_engineered = features_engineered.sort_values(['farm_id', 'date'])\n",
    "\n",
    "# Previous disease status (1 time step lag)\n",
    "features_engineered['prev_disease_label'] = features_engineered.groupby('farm_id')['disease_label'].shift(1)\n",
    "features_engineered['prev_severity'] = features_engineered.groupby('farm_id')['severity'].shift(1)\n",
    "features_engineered['prev_is_diseased'] = features_engineered.groupby('farm_id')['is_diseased'].shift(1)\n",
    "\n",
    "# Fill NaN values for first time step\n",
    "features_engineered['prev_disease_label'].fillna(0, inplace=True)  # Assume healthy initially\n",
    "features_engineered['prev_severity'].fillna(0, inplace=True)\n",
    "features_engineered['prev_is_diseased'].fillna(0, inplace=True)\n",
    "\n",
    "# Disease history (cumulative)\n",
    "features_engineered['disease_history_count'] = features_engineered.groupby('farm_id')['is_diseased'].cumsum()\n",
    "features_engineered['avg_historical_severity'] = features_engineered.groupby('farm_id')['severity'].expanding().mean().reset_index(0, drop=True)\n",
    "\n",
    "print(\"âœ… Created historical disease features\")\n",
    "\n",
    "# Update feature columns list\n",
    "new_feature_columns = [col for col in features_engineered.columns if col not in exclude_cols]\n",
    "print(f\"\\nTotal engineered features: {len(new_feature_columns)}\")\n",
    "print(f\"Added {len(new_feature_columns) - len(feature_columns)} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edge Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge features for graph construction\n",
    "print(\"Engineering edge features...\")\n",
    "\n",
    "# Calculate distance matrix\n",
    "distance_matrix = create_distance_matrix(farms_df)\n",
    "print(f\"âœ… Distance matrix created: {distance_matrix.shape}\")\n",
    "\n",
    "# Calculate environmental similarity for each time point\n",
    "time_points = sorted(features_engineered['date'].unique())\n",
    "edge_features_by_time = {}\n",
    "\n",
    "for time_point in time_points:\n",
    "    print(f\"Processing time point: {time_point}\")\n",
    "    \n",
    "    # Get data for this time point\n",
    "    time_data = features_engineered[features_engineered['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Environmental similarity based on weather features\n",
    "    weather_features = [col for col in time_data.columns if col.startswith('weather_')]\n",
    "    if weather_features:\n",
    "        weather_data = time_data[weather_features].values\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        env_similarity = cosine_similarity(weather_data)\n",
    "    else:\n",
    "        env_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Vegetation similarity\n",
    "    vegetation_features = [col for col in time_data.columns if col.startswith('vegetation_')]\n",
    "    if vegetation_features:\n",
    "        vegetation_data = time_data[vegetation_features].values\n",
    "        vegetation_similarity = cosine_similarity(vegetation_data)\n",
    "    else:\n",
    "        vegetation_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Crop type similarity\n",
    "    crop_features = [col for col in time_data.columns if col.startswith('crop_')]\n",
    "    if crop_features:\n",
    "        crop_data = time_data[crop_features].values\n",
    "        crop_similarity = cosine_similarity(crop_data)\n",
    "    else:\n",
    "        crop_similarity = np.ones((len(time_data), len(time_data)))\n",
    "    \n",
    "    # Combine edge features\n",
    "    edge_features = {\n",
    "        'distance': distance_matrix,\n",
    "        'environmental_similarity': env_similarity,\n",
    "        'vegetation_similarity': vegetation_similarity,\n",
    "        'crop_similarity': crop_similarity,\n",
    "        'combined_similarity': (env_similarity + vegetation_similarity + crop_similarity) / 3\n",
    "    }\n",
    "    \n",
    "    edge_features_by_time[time_point] = edge_features\n",
    "\n",
    "print(f\"âœ… Edge features created for {len(time_points)} time points\")\n",
    "print(f\"Edge feature types: {list(edge_features.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for disease prediction\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Prepare data for feature selection\n",
    "feature_cols = [col for col in features_engineered.columns if col not in exclude_cols]\n",
    "X = features_engineered[feature_cols].fillna(0)\n",
    "y = features_engineered['disease_label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Remove constant features\n",
    "constant_features = []\n",
    "for col in X.columns:\n",
    "    if X[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "if constant_features:\n",
    "    print(f\"Removing {len(constant_features)} constant features\")\n",
    "    X = X.drop(columns=constant_features)\n",
    "\n",
    "# Feature selection using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=min(50, X.shape[1]))  # Select top 50 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"\\nâœ… Selected {len(selected_features)} most important features\")\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance_score': feature_scores\n",
    "}).sort_values('importance_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "display(feature_importance.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance_score'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score (F-statistic)')\n",
    "plt.title('Top 20 Feature Importance for Disease Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '04_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(PROCESSED_DATA_DIR / 'feature_importance.csv', index=False)\n",
    "print(f\"âœ… Feature importance saved to {PROCESSED_DATA_DIR / 'feature_importance.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Final Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final feature matrices for GNN training\n",
    "print(\"Creating final feature matrices...\")\n",
    "\n",
    "# Use selected features for final matrix\n",
    "final_features = features_engineered[['farm_id', 'date'] + selected_features + \n",
    "                                   ['disease_type', 'disease_label', 'severity', 'is_diseased']].copy()\n",
    "\n",
    "print(f\"Final feature matrix shape: {final_features.shape}\")\n",
    "\n",
    "# Scale the selected features\n",
    "scaler = StandardScaler()\n",
    "final_features[selected_features] = scaler.fit_transform(final_features[selected_features])\n",
    "\n",
    "print(\"âœ… Features scaled using StandardScaler\")\n",
    "\n",
    "# Create node feature matrices for each time point\n",
    "node_features_by_time = {}\n",
    "labels_by_time = {}\n",
    "\n",
    "for time_point in time_points:\n",
    "    time_data = final_features[final_features['date'] == time_point].copy()\n",
    "    time_data = time_data.sort_values('farm_id').reset_index(drop=True)\n",
    "    \n",
    "    # Node features (exclude metadata and labels)\n",
    "    node_features = time_data[selected_features].values\n",
    "    labels = time_data['disease_label'].values\n",
    "    \n",
    "    node_features_by_time[time_point] = node_features\n",
    "    labels_by_time[time_point] = labels\n",
    "\n",
    "print(f\"âœ… Node features created for {len(time_points)} time points\")\n",
    "print(f\"Node feature dimension: {node_features.shape[1]}\")\n",
    "\n",
    "# Save processed features\n",
    "final_features.to_csv(PROCESSED_DATA_DIR / 'final_features.csv', index=False)\n",
    "print(f\"âœ… Final features saved to {PROCESSED_DATA_DIR / 'final_features.csv'}\")\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, PROCESSED_DATA_DIR / 'feature_scaler.pkl')\n",
    "print(f\"âœ… Feature scaler saved to {PROCESSED_DATA_DIR / 'feature_scaler.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns in features\n",
    "print(\"Analyzing temporal patterns...\")\n",
    "\n",
    "# Calculate feature stability over time\n",
    "feature_stability = {}\n",
    "\n",
    "for feature in selected_features[:10]:  # Analyze top 10 features\n",
    "    feature_values_by_time = []\n",
    "    \n",
    "    for time_point in time_points:\n",
    "        time_data = final_features[final_features['date'] == time_point]\n",
    "        feature_values_by_time.append(time_data[feature].mean())\n",
    "    \n",
    "    # Calculate coefficient of variation (stability measure)\n",
    "    cv = np.std(feature_values_by_time) / (np.mean(feature_values_by_time) + 1e-8)\n",
    "    feature_stability[feature] = cv\n",
    "\n",
    "# Visualize temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Disease progression over time\n",
    "disease_progression = final_features.groupby('date')['is_diseased'].mean()\n",
    "axes[0, 0].plot(disease_progression.index, disease_progression.values, 'o-', color='red')\n",
    "axes[0, 0].set_title('Disease Prevalence Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Proportion of Diseased Farms')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Feature stability\n",
    "stability_df = pd.DataFrame(list(feature_stability.items()), columns=['Feature', 'CV'])\n",
    "stability_df = stability_df.sort_values('CV')\n",
    "axes[0, 1].barh(range(len(stability_df)), stability_df['CV'])\n",
    "axes[0, 1].set_yticks(range(len(stability_df)))\n",
    "axes[0, 1].set_yticklabels(stability_df['Feature'])\n",
    "axes[0, 1].set_xlabel('Coefficient of Variation')\n",
    "axes[0, 1].set_title('Feature Temporal Stability')\n",
    "\n",
    "# 3. Vegetation health over time\n",
    "if 'vegetation_health_score' in final_features.columns:\n",
    "    veg_health = final_features.groupby('date')['vegetation_health_score'].mean()\n",
    "    axes[1, 0].plot(veg_health.index, veg_health.values, 'o-', color='green')\n",
    "    axes[1, 0].set_title('Average Vegetation Health Over Time')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Vegetation Health Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Weather stress over time\n",
    "if 'temperature_stress' in final_features.columns:\n",
    "    temp_stress = final_features.groupby('date')['temperature_stress'].mean()\n",
    "    axes[1, 1].plot(temp_stress.index, temp_stress.values, 'o-', color='orange')\n",
    "    axes[1, 1].set_title('Average Temperature Stress Over Time')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Temperature Stress')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '04_temporal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Temporal analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all engineered data for next notebooks\n",
    "print(\"Saving engineered features...\")\n",
    "\n",
    "# Save node features and labels by time\n",
    "import pickle\n",
    "\n",
    "feature_data = {\n",
    "    'node_features_by_time': node_features_by_time,\n",
    "    'labels_by_time': labels_by_time,\n",
    "    'edge_features_by_time': edge_features_by_time,\n",
    "    'selected_features': selected_features,\n",
    "    'feature_importance': feature_importance,\n",
    "    'time_points': time_points,\n",
    "    'farms_df': farms_df,\n",
    "    'distance_matrix': distance_matrix\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DATA_DIR / 'engineered_features.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_data, f)\n",
    "\n",
    "print(f\"âœ… Engineered features saved to {PROCESSED_DATA_DIR / 'engineered_features.pkl'}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_features': len(selected_features),\n",
    "    'total_farms': len(farms_df),\n",
    "    'total_time_points': len(time_points),\n",
    "    'disease_classes': len(final_features['disease_type'].unique()),\n",
    "    'feature_dimension': node_features.shape[1],\n",
    "    'total_samples': len(final_features)\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DATA_DIR / 'feature_summary.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Summary statistics saved to {PROCESSED_DATA_DIR / 'feature_summary.json'}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Feature engineering completed successfully!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"- Total engineered features: {len(selected_features)}\")\n",
    "print(f\"- Node feature dimension: {node_features.shape[1]}\")\n",
    "print(f\"- Time points: {len(time_points)}\")\n",
    "print(f\"- Total samples: {len(final_features)}\")\n",
    "print(f\"- Disease classes: {len(final_features['disease_type'].unique())}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run notebook 05_model_development.ipynb\")\n",
    "print(\"2. Train GNN models using engineered features\")\n",
    "print(\"3. Compare with baseline models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}