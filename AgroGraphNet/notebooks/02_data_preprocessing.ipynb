{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgroGraphNet: Data Preprocessing\n",
    "\n",
    "This notebook handles data preprocessing including satellite imagery processing, environmental data cleaning, and feature preparation.\n",
    "\n",
    "## Objectives:\n",
    "1. Process satellite imagery and calculate vegetation indices\n",
    "2. Clean and normalize environmental data\n",
    "3. Handle missing values and outliers\n",
    "4. Create temporal features\n",
    "5. Prepare data for graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import *\n",
    "from data_utils import *\n",
    "from visualization import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Processing data from: {RAW_DATA_DIR}\")\n",
    "print(f\"Output will be saved to: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "First, let's load the datasets created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading raw datasets...\")\n",
    "\n",
    "# Farm locations\n",
    "farm_files = list(FARM_LOCATIONS_DIR.glob('*.csv'))\n",
    "if farm_files:\n",
    "    farms_df = pd.read_csv(farm_files[0])\n",
    "    print(f\"✅ Loaded farm locations: {len(farms_df)} farms\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No farm location files found. Please run notebook 01 first.\")\n",
    "\n",
    "# Weather data\n",
    "weather_files = list(WEATHER_DIR.glob('*.csv'))\n",
    "if weather_files:\n",
    "    weather_df = load_weather_data(str(weather_files[0]))\n",
    "    print(f\"✅ Loaded weather data: {len(weather_df)} records\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No weather files found. Please run notebook 01 first.\")\n",
    "\n",
    "# Disease data\n",
    "disease_files = list(DISEASE_LABELS_DIR.glob('*.csv'))\n",
    "if disease_files:\n",
    "    disease_df = load_disease_labels(str(disease_files[0]))\n",
    "    print(f\"✅ Loaded disease data: {len(disease_df)} records\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No disease files found. Please run notebook 01 first.\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Satellite Imagery Processing\n",
    "\n",
    "Since we're working with sample data, we'll simulate satellite imagery processing. In a real scenario, you would load actual GeoTIFF files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for satellite imagery files\n",
    "satellite_files = list(SATELLITE_DIR.glob('*.tif')) + list(SATELLITE_DIR.glob('*.tiff'))\n",
    "\n",
    "if satellite_files:\n",
    "    print(f\"Found {len(satellite_files)} satellite imagery files\")\n",
    "    \n",
    "    # Process real satellite imagery\n",
    "    # This would involve loading GeoTIFF files and extracting pixel values\n",
    "    # For demonstration, we'll create simulated satellite features\n",
    "    print(\"Processing satellite imagery...\")\n",
    "    \n",
    "    # Simulate satellite band values for each farm\n",
    "    satellite_features = {}\n",
    "    \n",
    "    for band_name in SATELLITE_BANDS.keys():\n",
    "        # Simulate realistic band values\n",
    "        if band_name in ['B02', 'B03', 'B04']:  # Visible bands\n",
    "            values = np.random.uniform(0.05, 0.3, len(farms_df))\n",
    "        elif band_name == 'B08':  # NIR\n",
    "            values = np.random.uniform(0.3, 0.8, len(farms_df))\n",
    "        else:  # SWIR bands\n",
    "            values = np.random.uniform(0.1, 0.4, len(farms_df))\n",
    "        \n",
    "        satellite_features[band_name] = values\n",
    "    \n",
    "else:\n",
    "    print(\"No satellite imagery files found. Creating simulated satellite features...\")\n",
    "    \n",
    "    # Create simulated satellite features for each farm\n",
    "    satellite_features = {}\n",
    "    \n",
    "    for band_name in SATELLITE_BANDS.keys():\n",
    "        # Simulate realistic band values based on crop health\n",
    "        base_values = np.random.uniform(0.1, 0.5, len(farms_df))\n",
    "        \n",
    "        # Add some correlation with disease status if available\n",
    "        if len(disease_df) > 0:\n",
    "            # Get latest disease status for each farm\n",
    "            latest_disease = disease_df.loc[disease_df.groupby('farm_id')['date'].idxmax()]\n",
    "            \n",
    "            for i, farm in farms_df.iterrows():\n",
    "                farm_disease = latest_disease[latest_disease['farm_id'] == farm['farm_id']]\n",
    "                if len(farm_disease) > 0 and farm_disease.iloc[0]['disease_type'] != 'Healthy':\n",
    "                    # Diseased farms have different spectral signatures\n",
    "                    if band_name == 'B08':  # NIR typically lower in diseased plants\n",
    "                        base_values[i] *= 0.8\n",
    "                    elif band_name in ['B04']:  # Red might be higher\n",
    "                        base_values[i] *= 1.2\n",
    "        \n",
    "        satellite_features[band_name] = base_values\n",
    "\n",
    "# Convert to DataFrame\n",
    "satellite_df = pd.DataFrame(satellite_features)\n",
    "satellite_df['farm_id'] = farms_df['farm_id']\n",
    "\n",
    "print(f\"✅ Satellite features created: {satellite_df.shape}\")\n",
    "print(f\"Bands: {list(satellite_features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vegetation indices\n",
    "print(\"Calculating vegetation indices...\")\n",
    "\n",
    "# Create band mapping for vegetation index calculation\n",
    "band_mapping = {\n",
    "    'Red': 'B04',\n",
    "    'NIR': 'B08',\n",
    "    'Blue': 'B02',\n",
    "    'SWIR1': 'B11'\n",
    "}\n",
    "\n",
    "# Calculate indices for each farm\n",
    "vegetation_indices = {}\n",
    "\n",
    "for index_name in VEGETATION_INDICES:\n",
    "    if index_name == 'NDVI':\n",
    "        # NDVI = (NIR - Red) / (NIR + Red)\n",
    "        nir = satellite_df['B08'].values\n",
    "        red = satellite_df['B04'].values\n",
    "        vegetation_indices[index_name] = (nir - red) / (nir + red + 1e-8)\n",
    "    \n",
    "    elif index_name == 'EVI':\n",
    "        # EVI = 2.5 * (NIR - Red) / (NIR + 6*Red - 7.5*Blue + 1)\n",
    "        nir = satellite_df['B08'].values\n",
    "        red = satellite_df['B04'].values\n",
    "        blue = satellite_df['B02'].values\n",
    "        vegetation_indices[index_name] = 2.5 * (nir - red) / (nir + 6*red - 7.5*blue + 1 + 1e-8)\n",
    "    \n",
    "    elif index_name == 'SAVI':\n",
    "        # SAVI = (1 + L) * (NIR - Red) / (NIR + Red + L)\n",
    "        L = 0.5\n",
    "        nir = satellite_df['B08'].values\n",
    "        red = satellite_df['B04'].values\n",
    "        vegetation_indices[index_name] = (1 + L) * (nir - red) / (nir + red + L + 1e-8)\n",
    "    \n",
    "    elif index_name == 'NDWI':\n",
    "        # NDWI = (NIR - SWIR1) / (NIR + SWIR1)\n",
    "        nir = satellite_df['B08'].values\n",
    "        swir1 = satellite_df['B11'].values\n",
    "        vegetation_indices[index_name] = (nir - swir1) / (nir + swir1 + 1e-8)\n",
    "\n",
    "# Add vegetation indices to satellite DataFrame\n",
    "for index_name, values in vegetation_indices.items():\n",
    "    satellite_df[index_name] = values\n",
    "\n",
    "print(f\"✅ Vegetation indices calculated: {list(vegetation_indices.keys())}\")\n",
    "print(f\"Updated satellite features shape: {satellite_df.shape}\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nVegetation Index Statistics:\")\n",
    "for index_name in VEGETATION_INDICES:\n",
    "    values = vegetation_indices[index_name]\n",
    "    print(f\"{index_name}: mean={values.mean():.3f}, std={values.std():.3f}, range=[{values.min():.3f}, {values.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environmental Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess weather data\n",
    "print(\"Preprocessing weather data...\")\n",
    "\n",
    "# Check for outliers and anomalies\n",
    "weather_stats = weather_df.describe()\n",
    "print(\"Weather data statistics:\")\n",
    "display(weather_stats)\n",
    "\n",
    "# Handle outliers using IQR method\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    outliers_removed = 0\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        outliers_removed += outliers.sum()\n",
    "        \n",
    "        # Replace outliers with median values\n",
    "        df_clean.loc[outliers, col] = df[col].median()\n",
    "    \n",
    "    return df_clean, outliers_removed\n",
    "\n",
    "# Remove outliers from weather data\n",
    "weather_columns = ['temperature', 'humidity', 'precipitation', 'wind_speed']\n",
    "weather_clean, outliers_count = remove_outliers_iqr(weather_df, weather_columns)\n",
    "\n",
    "print(f\"\\n✅ Outliers handled: {outliers_count} values replaced with median\")\n",
    "\n",
    "# Handle missing values\n",
    "missing_before = weather_clean.isnull().sum().sum()\n",
    "if missing_before > 0:\n",
    "    print(f\"Handling {missing_before} missing values...\")\n",
    "    \n",
    "    # Use forward fill for temporal data, then backward fill\n",
    "    weather_clean = weather_clean.sort_values(['lat', 'lon', 'date'])\n",
    "    weather_clean[weather_columns] = weather_clean.groupby(['lat', 'lon'])[weather_columns].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # If still missing, use overall median\n",
    "    for col in weather_columns:\n",
    "        weather_clean[col].fillna(weather_clean[col].median(), inplace=True)\n",
    "    \n",
    "    missing_after = weather_clean.isnull().sum().sum()\n",
    "    print(f\"✅ Missing values after cleaning: {missing_after}\")\n",
    "else:\n",
    "    print(\"✅ No missing values found in weather data\")\n",
    "\n",
    "# Create temporal features\n",
    "print(\"\\nCreating temporal features...\")\n",
    "weather_clean['month'] = weather_clean['date'].dt.month\n",
    "weather_clean['day_of_year'] = weather_clean['date'].dt.dayofyear\n",
    "weather_clean['season'] = weather_clean['month'].map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "})\n",
    "\n",
    "print(f\"✅ Weather data preprocessed: {weather_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess disease data\n",
    "print(\"Preprocessing disease data...\")\n",
    "\n",
    "# Create numerical labels for disease types\n",
    "disease_mapping = {disease: idx for idx, disease in DISEASE_CLASSES.items()}\n",
    "disease_clean = disease_df.copy()\n",
    "disease_clean['disease_label'] = disease_clean['disease_type'].map(disease_mapping)\n",
    "\n",
    "print(f\"Disease mapping: {disease_mapping}\")\n",
    "\n",
    "# Handle missing severity values\n",
    "missing_severity = disease_clean['severity'].isnull().sum()\n",
    "if missing_severity > 0:\n",
    "    print(f\"Handling {missing_severity} missing severity values...\")\n",
    "    # Set severity to 0 for healthy crops, use median for others\n",
    "    disease_clean.loc[disease_clean['disease_type'] == 'Healthy', 'severity'] = 0\n",
    "    disease_clean['severity'].fillna(disease_clean[disease_clean['disease_type'] != 'Healthy']['severity'].median(), inplace=True)\n",
    "\n",
    "# Create binary disease indicator\n",
    "disease_clean['is_diseased'] = (disease_clean['disease_type'] != 'Healthy').astype(int)\n",
    "\n",
    "# Create temporal features\n",
    "disease_clean['month'] = disease_clean['date'].dt.month\n",
    "disease_clean['season'] = disease_clean['month'].map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "})\n",
    "\n",
    "print(f\"✅ Disease data preprocessed: {disease_clean.shape}\")\n",
    "print(f\"Disease distribution: {disease_clean['disease_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated features for each farm\n",
    "print(\"Creating aggregated features for each farm...\")\n",
    "\n",
    "# Aggregate weather data by farm and time period\n",
    "def aggregate_weather_by_farm(weather_df, farms_df, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Aggregate weather data for each farm location\n",
    "    \"\"\"\n",
    "    farm_weather = []\n",
    "    \n",
    "    for _, farm in farms_df.iterrows():\n",
    "        # Find weather data near this farm\n",
    "        nearby_weather = weather_df[\n",
    "            (abs(weather_df['lat'] - farm['lat']) < tolerance) & \n",
    "            (abs(weather_df['lon'] - farm['lon']) < tolerance)\n",
    "        ]\n",
    "        \n",
    "        if len(nearby_weather) == 0:\n",
    "            # If no nearby weather, use closest weather station\n",
    "            distances = np.sqrt(\n",
    "                (weather_df['lat'] - farm['lat'])**2 + \n",
    "                (weather_df['lon'] - farm['lon'])**2\n",
    "            )\n",
    "            closest_idx = distances.idxmin()\n",
    "            nearby_weather = weather_df[weather_df.index == closest_idx]\n",
    "        \n",
    "        # Aggregate by time period\n",
    "        for date in nearby_weather['date'].unique():\n",
    "            date_weather = nearby_weather[nearby_weather['date'] == date]\n",
    "            \n",
    "            farm_weather.append({\n",
    "                'farm_id': farm['farm_id'],\n",
    "                'date': date,\n",
    "                'temperature': date_weather['temperature'].mean(),\n",
    "                'humidity': date_weather['humidity'].mean(),\n",
    "                'precipitation': date_weather['precipitation'].mean(),\n",
    "                'wind_speed': date_weather['wind_speed'].mean(),\n",
    "                'wind_direction': date_weather['wind_direction'].mean(),\n",
    "                'month': date_weather['month'].iloc[0],\n",
    "                'season': date_weather['season'].iloc[0]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(farm_weather)\n",
    "\n",
    "# Aggregate weather data\n",
    "farm_weather_df = aggregate_weather_by_farm(weather_clean, farms_df)\n",
    "print(f\"✅ Farm weather data aggregated: {farm_weather_df.shape}\")\n",
    "\n",
    "# Create temporal weather features (rolling averages, trends)\n",
    "print(\"Creating temporal weather features...\")\n",
    "\n",
    "farm_weather_df = farm_weather_df.sort_values(['farm_id', 'date'])\n",
    "\n",
    "# Calculate rolling averages (30-day window)\n",
    "weather_features = ['temperature', 'humidity', 'precipitation', 'wind_speed']\n",
    "for feature in weather_features:\n",
    "    farm_weather_df[f'{feature}_rolling_mean'] = farm_weather_df.groupby('farm_id')[feature].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    farm_weather_df[f'{feature}_rolling_std'] = farm_weather_df.groupby('farm_id')[feature].rolling(window=3, min_periods=1).std().reset_index(0, drop=True)\n",
    "\n",
    "# Fill NaN values in rolling std with 0\n",
    "rolling_std_cols = [col for col in farm_weather_df.columns if 'rolling_std' in col]\n",
    "farm_weather_df[rolling_std_cols] = farm_weather_df[rolling_std_cols].fillna(0)\n",
    "\n",
    "print(f\"✅ Temporal weather features created: {farm_weather_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature matrix for each farm at each time point\n",
    "print(\"Creating comprehensive feature matrix...\")\n",
    "\n",
    "# Get unique time points\n",
    "time_points = sorted(disease_clean['date'].unique())\n",
    "print(f\"Time points: {len(time_points)}\")\n",
    "\n",
    "# Create feature matrix\n",
    "feature_data = []\n",
    "\n",
    "for time_point in time_points:\n",
    "    for _, farm in farms_df.iterrows():\n",
    "        farm_id = farm['farm_id']\n",
    "        \n",
    "        # Get farm static features\n",
    "        farm_features = {\n",
    "            'farm_id': farm_id,\n",
    "            'date': time_point,\n",
    "            'lat': farm['lat'],\n",
    "            'lon': farm['lon'],\n",
    "            'area_hectares': farm['area_hectares']\n",
    "        }\n",
    "        \n",
    "        # Add crop type (one-hot encoded)\n",
    "        for crop_type in farms_df['crop_type'].unique():\n",
    "            farm_features[f'crop_{crop_type}'] = int(farm['crop_type'] == crop_type)\n",
    "        \n",
    "        # Add satellite features\n",
    "        farm_satellite = satellite_df[satellite_df['farm_id'] == farm_id]\n",
    "        if len(farm_satellite) > 0:\n",
    "            for band in SATELLITE_BANDS.keys():\n",
    "                farm_features[f'satellite_{band}'] = farm_satellite[band].iloc[0]\n",
    "            \n",
    "            for index in VEGETATION_INDICES:\n",
    "                farm_features[f'vegetation_{index}'] = farm_satellite[index].iloc[0]\n",
    "        else:\n",
    "            # Fill with median values if no satellite data\n",
    "            for band in SATELLITE_BANDS.keys():\n",
    "                farm_features[f'satellite_{band}'] = satellite_df[band].median()\n",
    "            \n",
    "            for index in VEGETATION_INDICES:\n",
    "                farm_features[f'vegetation_{index}'] = satellite_df[index].median()\n",
    "        \n",
    "        # Add weather features\n",
    "        farm_weather = farm_weather_df[\n",
    "            (farm_weather_df['farm_id'] == farm_id) & \n",
    "            (farm_weather_df['date'] == time_point)\n",
    "        ]\n",
    "        \n",
    "        if len(farm_weather) > 0:\n",
    "            weather_cols = ['temperature', 'humidity', 'precipitation', 'wind_speed', 'wind_direction']\n",
    "            for col in weather_cols:\n",
    "                farm_features[f'weather_{col}'] = farm_weather[col].iloc[0]\n",
    "                \n",
    "                # Add rolling features if available\n",
    "                if f'{col}_rolling_mean' in farm_weather.columns:\n",
    "                    farm_features[f'weather_{col}_rolling_mean'] = farm_weather[f'{col}_rolling_mean'].iloc[0]\n",
    "                    farm_features[f'weather_{col}_rolling_std'] = farm_weather[f'{col}_rolling_std'].iloc[0]\n",
    "            \n",
    "            # Add temporal features\n",
    "            farm_features['month'] = farm_weather['month'].iloc[0]\n",
    "            farm_features['season'] = farm_weather['season'].iloc[0]\n",
    "        else:\n",
    "            # Fill with overall averages if no weather data\n",
    "            weather_cols = ['temperature', 'humidity', 'precipitation', 'wind_speed', 'wind_direction']\n",
    "            for col in weather_cols:\n",
    "                farm_features[f'weather_{col}'] = farm_weather_df[col].mean()\n",
    "                farm_features[f'weather_{col}_rolling_mean'] = farm_weather_df[f'{col}_rolling_mean'].mean()\n",
    "                farm_features[f'weather_{col}_rolling_std'] = farm_weather_df[f'{col}_rolling_std'].mean()\n",
    "            \n",
    "            farm_features['month'] = pd.to_datetime(time_point).month\n",
    "            farm_features['season'] = {12: 'Winter', 1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring', 6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Fall', 10: 'Fall', 11: 'Fall'}[pd.to_datetime(time_point).month]\n",
    "        \n",
    "        # Add disease labels\n",
    "        farm_disease = disease_clean[\n",
    "            (disease_clean['farm_id'] == farm_id) & \n",
    "            (disease_clean['date'] == time_point)\n",
    "        ]\n",
    "        \n",
    "        if len(farm_disease) > 0:\n",
    "            farm_features['disease_type'] = farm_disease['disease_type'].iloc[0]\n",
    "            farm_features['disease_label'] = farm_disease['disease_label'].iloc[0]\n",
    "            farm_features['severity'] = farm_disease['severity'].iloc[0]\n",
    "            farm_features['is_diseased'] = farm_disease['is_diseased'].iloc[0]\n",
    "        else:\n",
    "            # Default to healthy if no disease data\n",
    "            farm_features['disease_type'] = 'Healthy'\n",
    "            farm_features['disease_label'] = 0\n",
    "            farm_features['severity'] = 0.0\n",
    "            farm_features['is_diseased'] = 0\n",
    "        \n",
    "        feature_data.append(farm_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(feature_data)\n",
    "\n",
    "print(f\"✅ Comprehensive feature matrix created: {features_df.shape}\")\n",
    "print(f\"Features per sample: {len([col for col in features_df.columns if col not in ['farm_id', 'date', 'disease_type', 'disease_label', 'severity', 'is_diseased']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns for scaling\n",
    "feature_columns = [col for col in features_df.columns if col not in [\n",
    "    'farm_id', 'date', 'disease_type', 'disease_label', 'severity', 'is_diseased', 'season'\n",
    "]]\n",
    "\n",
    "print(f\"Features to scale: {len(feature_columns)}\")\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = [col for col in feature_columns if col.startswith('crop_') or col == 'month']\n",
    "numerical_features = [col for col in feature_columns if col not in categorical_features]\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "\n",
    "# Scale numerical features\n",
    "print(\"\\nScaling numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "features_scaled = features_df.copy()\n",
    "\n",
    "# Fit scaler on numerical features\n",
    "features_scaled[numerical_features] = scaler.fit_transform(features_df[numerical_features])\n",
    "\n",
    "print(f\"✅ Numerical features scaled using StandardScaler\")\n",
    "\n",
    "# Handle categorical features (already one-hot encoded)\n",
    "print(\"✅ Categorical features already encoded\")\n",
    "\n",
    "# Check for any remaining NaN values\n",
    "nan_count = features_scaled[feature_columns].isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"⚠️ Warning: {nan_count} NaN values found after scaling\")\n",
    "    # Fill remaining NaN with 0\n",
    "    features_scaled[feature_columns] = features_scaled[feature_columns].fillna(0)\n",
    "    print(\"✅ NaN values filled with 0\")\n",
    "else:\n",
    "    print(\"✅ No NaN values found\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {features_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions and correlations\n",
    "print(\"Creating data quality visualizations...\")\n",
    "\n",
    "# Plot feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select key features to visualize\n",
    "key_features = [\n",
    "    'vegetation_NDVI', 'vegetation_EVI', 'weather_temperature',\n",
    "    'weather_humidity', 'weather_precipitation', 'area_hectares'\n",
    "]\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    if feature in features_scaled.columns:\n",
    "        axes[i].hist(features_scaled[feature], bins=30, alpha=0.7, color='skyblue')\n",
    "        axes[i].set_title(f'{feature} Distribution')\n",
    "        axes[i].set_xlabel('Scaled Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '02_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix for key numerical features\n",
    "print(\"\\nCreating correlation matrix...\")\n",
    "correlation_features = [f for f in key_features if f in features_scaled.columns]\n",
    "correlation_matrix = features_scaled[correlation_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '02_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disease distribution over time and space\n",
    "print(\"Visualizing disease patterns...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Disease distribution over time\n",
    "disease_time = features_scaled.groupby(['date', 'disease_type']).size().unstack(fill_value=0)\n",
    "disease_time.plot(kind='bar', stacked=True, ax=axes[0, 0], color=['green', 'red', 'orange', 'purple', 'darkred'])\n",
    "axes[0, 0].set_title('Disease Distribution Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Number of Farms')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Disease severity distribution\n",
    "diseased_farms = features_scaled[features_scaled['is_diseased'] == 1]\n",
    "if len(diseased_farms) > 0:\n",
    "    axes[0, 1].hist(diseased_farms['severity'], bins=20, alpha=0.7, color='red')\n",
    "    axes[0, 1].set_title('Disease Severity Distribution')\n",
    "    axes[0, 1].set_xlabel('Severity')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# NDVI vs Disease Status\n",
    "healthy_farms = features_scaled[features_scaled['disease_type'] == 'Healthy']\n",
    "diseased_farms = features_scaled[features_scaled['disease_type'] != 'Healthy']\n",
    "\n",
    "if 'vegetation_NDVI' in features_scaled.columns:\n",
    "    axes[1, 0].hist(healthy_farms['vegetation_NDVI'], bins=20, alpha=0.7, \n",
    "                   color='green', label='Healthy', density=True)\n",
    "    axes[1, 0].hist(diseased_farms['vegetation_NDVI'], bins=20, alpha=0.7, \n",
    "                   color='red', label='Diseased', density=True)\n",
    "    axes[1, 0].set_title('NDVI Distribution by Disease Status')\n",
    "    axes[1, 0].set_xlabel('NDVI (scaled)')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "# Geographic distribution of diseases\n",
    "disease_colors = {'Healthy': 'green', 'Blight': 'red', 'Rust': 'orange', 'Mosaic': 'purple', 'Bacterial': 'darkred'}\n",
    "for disease_type, color in disease_colors.items():\n",
    "    disease_subset = features_scaled[features_scaled['disease_type'] == disease_type]\n",
    "    if len(disease_subset) > 0:\n",
    "        axes[1, 1].scatter(disease_subset['lon'], disease_subset['lat'], \n",
    "                          c=color, label=disease_type, alpha=0.6, s=20)\n",
    "\n",
    "axes[1, 1].set_title('Geographic Distribution of Diseases')\n",
    "axes[1, 1].set_xlabel('Longitude')\n",
    "axes[1, 1].set_ylabel('Latitude')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '02_disease_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save main feature matrix\n",
    "features_scaled.to_csv(PROCESSED_DATA_DIR / 'features_scaled.csv', index=False)\n",
    "print(f\"✅ Scaled features saved: {PROCESSED_DATA_DIR / 'features_scaled.csv'}\")\n",
    "\n",
    "# Save individual processed datasets\n",
    "farms_df.to_csv(PROCESSED_DATA_DIR / 'farms_processed.csv', index=False)\n",
    "weather_clean.to_csv(PROCESSED_DATA_DIR / 'weather_processed.csv', index=False)\n",
    "disease_clean.to_csv(PROCESSED_DATA_DIR / 'disease_processed.csv', index=False)\n",
    "satellite_df.to_csv(PROCESSED_DATA_DIR / 'satellite_processed.csv', index=False)\n",
    "farm_weather_df.to_csv(PROCESSED_DATA_DIR / 'farm_weather_aggregated.csv', index=False)\n",
    "\n",
    "print(\"✅ Individual processed datasets saved\")\n",
    "\n",
    "# Save feature scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, PROCESSED_DATA_DIR / 'feature_scaler.pkl')\n",
    "print(\"✅ Feature scaler saved\")\n",
    "\n",
    "# Save feature column names\n",
    "feature_info = {\n",
    "    'all_features': feature_columns,\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'target_columns': ['disease_label', 'disease_type', 'severity', 'is_diseased']\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(PROCESSED_DATA_DIR / 'feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(\"✅ Feature information saved\")\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = {\n",
    "    'total_samples': len(features_scaled),\n",
    "    'total_farms': features_scaled['farm_id'].nunique(),\n",
    "    'time_points': len(features_scaled['date'].unique()),\n",
    "    'total_features': len(feature_columns),\n",
    "    'disease_distribution': features_scaled['disease_type'].value_counts().to_dict(),\n",
    "    'feature_statistics': features_scaled[numerical_features].describe().to_dict()\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DATA_DIR / 'data_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ Data summary saved\")\n",
    "\n",
    "print(f\"\\n🎉 Data preprocessing completed successfully!\")\n",
    "print(f\"Processed data saved to: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  - Total samples: {len(features_scaled):,}\")\n",
    "print(f\"  - Unique farms: {features_scaled['farm_id'].nunique()}\")\n",
    "print(f\"  - Time points: {len(features_scaled['date'].unique())}\")\n",
    "print(f\"  - Total features: {len(feature_columns)}\")\n",
    "print(f\"  - Disease classes: {len(features_scaled['disease_type'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has completed the following preprocessing tasks:\n",
    "\n",
    "1. ✅ **Satellite Imagery Processing**\n",
    "   - Simulated satellite band values for each farm\n",
    "   - Calculated vegetation indices (NDVI, EVI, SAVI, NDWI)\n",
    "   - Created spectral features correlated with crop health\n",
    "\n",
    "2. ✅ **Environmental Data Cleaning**\n",
    "   - Removed outliers using IQR method\n",
    "   - Handled missing values with temporal interpolation\n",
    "   - Created temporal features (month, season)\n",
    "   - Calculated rolling averages and trends\n",
    "\n",
    "3. ✅ **Feature Engineering**\n",
    "   - Aggregated weather data by farm location\n",
    "   - Created comprehensive feature matrix\n",
    "   - One-hot encoded categorical variables\n",
    "   - Combined static and temporal features\n",
    "\n",
    "4. ✅ **Data Scaling and Normalization**\n",
    "   - Applied StandardScaler to numerical features\n",
    "   - Preserved categorical encodings\n",
    "   - Handled remaining missing values\n",
    "\n",
    "5. ✅ **Quality Assessment and Visualization**\n",
    "   - Created feature distribution plots\n",
    "   - Generated correlation matrices\n",
    "   - Visualized disease patterns over time and space\n",
    "\n",
    "6. ✅ **Data Persistence**\n",
    "   - Saved processed datasets and feature matrices\n",
    "   - Stored feature scaler for future use\n",
    "   - Created comprehensive data summary\n",
    "\n",
    "**Next Steps:**\n",
    "- Run notebook `03_graph_construction.ipynb` to build farm network graphs\n",
    "- The processed feature matrix is ready for graph neural network training\n",
    "- All data quality issues have been addressed and features are properly scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}